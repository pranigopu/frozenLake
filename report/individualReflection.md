# ECS7002P: _Artificial Intelligence in Games_<br>Assignment 2, Individual Reflection

## Student details

- **Name**: Pranav Narendra Gopalkrishna
- **Number**: 231052045

## Reflection

The method of collaboration in this project was to attempt the coding-related questions of the assignment individually before coming together to check each other's implementations, decide on the final implementation and make the report.

<br>Malo Hamon was the first to finish the required implementations; Nimesh Bansal and I occasionally referenced them to cross-check with our own implementations. Malo also wrote the code for computing and plotting moving averages of per-episode discounted returns, which I used to derive my own code for the first draft of the report. In summary, Malo's contributions were as follows:

- Implemented the `FrozenLake` environment
- Implemented policy and value iteration
- Implemented SARSA control and Q-learning
- Implemented linear function approximation methods for the above
- Implemented deep Q-network learning
- Wrote, executed and reported on optimising SARSA control and Q-learning parameters
- Documenting the final code's basic structure
- Rewrote and improved on the answer to question 4 of the report
- Reviewed questions 1, 2, 5 and 6 of the report
- Put together and tested the final code for submission

<br>I first did all of my implementations independently, with extensive comments that contributed to the implementation notes in the report. Cross-checking my code with Malo's after implementation brought to light key mistakes and improvements that led to further understanding, part of which was later added to the report. I wrote the first draft of the report, but questions 3 and 4 of the report were entirely reworked by Malo. I later used Malo's updated code to redo the graphs of question 2 of the report, and used his insights to correct the results of question 1. In summary, my contributions were as follows:

- Implemented the `FrozenLake` environment
- Implemented policy and value iteration
- Implemented SARSA control and Q-learning
- Implemented linear function approximation methods for the above
- Implemented and extensively documented deep Q-network learning
- Wrote the first draft of the report for all questions
- Wrote and expanded on questions 1, 2, 5 and 6 of the report
- Reviewed question 4 of the report
- Wrote the definitions section and implementation notes

<br>Nimesh Bansal worked on the implementations independently, but was unable to contribute as much to the final submission as me or Malo due to illness. In summary, Nimesh's work was as follows:

- Implemented the `FrozenLake` environment
- Implemented policy and value iteration
- Implemented SARSA control and Q-learning
- Implemented linear approximation methods for the above
- Implemented deep Q-network learning
