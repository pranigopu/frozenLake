{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 2:<br>**Tabular Model-based Methods**\n",
        "\n",
        "## Introduction to the concept\n",
        "Model-based methods are algorithms that use (thus require) a known environment model. \"Tabular\" refers to the use of look-up tables or arrays that store previous estimates of a value function (ex. state value function) and use these estimates to update them every iteration. It is not surprising that when the environment model is known, i.e. when the state-transition probabilities and rewards are known for every state-action pair, the evaluation an improvement of policies becomes much more efficient."
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Mounting the Google Drive folder (run if necessary):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    # Saving the present working directory's path:\n",
        "    pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "beb953ef-89bd-4f65-fde2-f45262d63401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import import_ipynb\n",
        "    N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "    N.load_module(\"Q1_environment\")\n",
        "    from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "65d2bae7-c25e-4f44-ec6a-64dbf3227017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "vglfX3kDrA8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Wms_fa7_rDCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy evaluation\n",
        "This function estimates the state value function (represented by an array of values per state) give a certain policy. Its goal is not to modify the policy but simply try to accurately obtain the value of each state. The value of a state is updated by the formula:\n",
        "\n",
        "$\\forall s \\in S, V(s) \\leftarrow V(s) + \\sum_{a \\in A} \\pi(s, a) \\sum_{s' \\in S} P(s'|s, a) (R(s'|s, a) + \\gamma V(s'))$\n",
        "\n",
        "Here, we have the following:\n",
        "\n",
        "- $S$: The set of all possible states\n",
        "- $A$: The set of all possible actions\n",
        "- $V$: The estimated value function\n",
        "- $\\pi$: The policy, which maps $s \\in S$ & $a \\in A$ to the probability of taking $a$ from $s$\n",
        "- $P(s'|s, a)$: The probability of transitioning to $s'$ having taking $a$ from $s$\n",
        "- $R(s'|s, a)$: The reward of transitioning to $s'$ having taking $a$ from $s$\n",
        "- $\\gamma$: The discount factor (a constant predefined parameter)\n",
        "\n",
        "<br>The policy evaluation function is designed to ensure that:<br>$\\forall s \\in S, V(s) \\rightarrow V^{\\pi}(s)$ as $i \\rightarrow \\infty$\n",
        "\n",
        "Here, $V^{\\pi}$ is the true value function for policy $\\pi$ & $i$ is the number of iterations of the algorithm."
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma, theta, max_iterations):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    `env`: Object of the chosen environment class (ex. `FrozenLake`)\n",
        "    `policy`: Array giving the probability of taking an action from a state\n",
        "    `gamma`: Discount factor\n",
        "    `theta`: Error tolerance level\n",
        "\n",
        "    FURTHER NOTE ON `policy`:\n",
        "    We consider the policy to be deterministic, meaning that it maps each state\n",
        "    to a certain action, rather than each state-action pair to a probability.\n",
        "    Hence, `policy` is a 1D array with each index corresponding to a state and\n",
        "    the value at a given index i corresponding to the action to be taken from\n",
        "    state i. This is equivalent to the policy wherein each state-action pair is\n",
        "    related to either 0 or 1 so that each state is mapped to 1 only when paired\n",
        "    with a particular action.\n",
        "    '''\n",
        "    # Initialising table of values per state:\n",
        "    value = np.zeros(env.n_states, dtype=float)\n",
        "    # Policy evaluation loop:\n",
        "    for i in range(max_iterations):\n",
        "        # Flag for indicating convergence of value evaluation:\n",
        "        flag = 0\n",
        "        for s in range(env.n_states):\n",
        "            # NOTE: s ==> state\n",
        "            # Storing previous value of value function:\n",
        "            v = value[s]\n",
        "            #------------------------------------\n",
        "            # Obtaining current value of value function:\n",
        "            # NOTE: We iterate through every possible action from state `s`\n",
        "            value[s] = float(0)\n",
        "            for a in range(env.n_actions):\n",
        "                # NOTE: a ==> action\n",
        "                # If policy does not map `s` to `a`, move to next action:\n",
        "                if policy[s] != a: continue\n",
        "                # If policy does map `s` to `a`, update state value:\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    value[s] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    `env.p`: Probability of moving `s` to `_s` given action `a`\n",
        "                    `env.r`: Reward of moving `s` to `_s` given action `a`\n",
        "                    '''\n",
        "            #------------------------------------\n",
        "            # Obtaining the difference in state value:\n",
        "            # NOTE: This is why we stored the previous value before in `v`\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "    return value"
      ],
      "metadata": {
        "id": "UedO36FhIPP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy improvement"
      ],
      "metadata": {
        "id": "roJtJVb5d87b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(env, value, gamma):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    `env`: Object of the chosen environment class (ex. `FrozenLake`)\n",
        "    `value`: Array containing values of each state with respect to some policy\n",
        "    `gamma`: Discount factor\n",
        "\n",
        "    NOTE ON POLICY IMPROVEMENT:\n",
        "    The goal of policy improvement is to improve on the previously used policy\n",
        "    (which is implicit in the array of state values, which is evaluated with\n",
        "    respect to some policy). We do this by choosing for each state `s` the\n",
        "    action `a` such that we maximise the reward of taking a from `s`\n",
        "    (irrespective of policy) then following the previous policy (which is\n",
        "    implicit in the array of state values).\n",
        "    '''\n",
        "    policy = np.zeros(env.n_states, dtype=int)\n",
        "    for s in range(env.n_states):\n",
        "        q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "        for a in range(env.n_actions):\n",
        "            for _s in range(env.n_states):\n",
        "                # NOTE: `_s` ==> next state\n",
        "                # Total reward of taking a from s then following last policy:\n",
        "                '''\n",
        "                NOTE ON LAST POLICY:\n",
        "                The previous policy based on which we are making the current\n",
        "                improvement is implicit in the array of state values `value`,\n",
        "                since this array was obtained with respect to some policy.\n",
        "                '''\n",
        "                q[a] += env.p(_s, s, a)*(env.r(_s, s, a) + gamma*value[_s])\n",
        "                '''\n",
        "                NOTE ON ABOVE USED FUNCTIONS:\n",
        "                `env.p`: Probability of moving `s` to _s given action `a`\n",
        "                `env.r`: Reward of moving `s` to `_s` given action `a`\n",
        "                '''\n",
        "        # Update policy to maximise the one-step dynamics from `s`:\n",
        "        policy[s] = np.argmax(q)\n",
        "    return policy"
      ],
      "metadata": {
        "id": "A4D_aCb4dvhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy iteration..."
      ],
      "metadata": {
        "id": "gO_Ul57neABk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma, theta, max_iterations, policy=None):\n",
        "    if policy is None: policy = np.zeros(env.n_states, dtype=int)\n",
        "    else: policy = np.array(policy, dtype=int)\n",
        "    # Initialising state values with respect to existing policy:\n",
        "    value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "    # Policy iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        policy = policy_improvement(env, value, gamma)\n",
        "        new_value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "        # If all value evaluations change less than `theta`, break:\n",
        "        if all(abs(new_value-value) < theta): break\n",
        "        # Else, continue improving with the newly evaluated state values:\n",
        "        value = new_value\n",
        "\n",
        "    # FORMAT OF RETURN: policy, state-values, iterations taken\n",
        "    return policy, value, i"
      ],
      "metadata": {
        "id": "kWqlg13zWAxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value iteration"
      ],
      "metadata": {
        "id": "w4oRY11heB7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma, theta, max_iterations, value=None):\n",
        "    if value is None: value = np.zeros(env.n_states)\n",
        "    else: value = np.array(value, dtype=np.float)\n",
        "    # Value iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        # Flag for indicating convergence of value evaluation:\n",
        "        flag = 0\n",
        "        for s in range(env.n_states):\n",
        "            q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "            for a in range(env.n_actions):\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    # Total reward of taking a from s for previous state value:\n",
        "                    q[a] += env.p(_s, s, a)*(env.r(_s, s, a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    `env.p`: Probability of moving `s` to `_s` given action `a`\n",
        "                    `env.r`: Reward of moving from `s` to `_s` given action `a`\n",
        "                    '''\n",
        "            # Update policy to maximise the one-step dynamics from s:\n",
        "            v = value[s]\n",
        "            value[s] = np.max(q)\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the (estimated) optimal policy:\n",
        "    # NOTE: The logic for this is identical to policy improvement\n",
        "    policy = policy_improvement(env, value, gamma)\n",
        "\n",
        "    # FORMAT OF RETURN: policy, state-values, iterations taken\n",
        "    return policy, value, i"
      ],
      "metadata": {
        "id": "zOCfwm_0d166"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "2jbrveZR4CXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake['small'], 0.1, 100)\n",
        "    gamma = 0.9\n",
        "    theta = 0.001\n",
        "    max_iterations = 100\n",
        "\n",
        "    # Running the functions:\n",
        "    PI = policy_iteration(env, gamma, theta, max_iterations)\n",
        "    VI = value_iteration(env, gamma, theta, max_iterations)\n",
        "    labels = (\"policy iteration\", \"value iteration\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults((PI, VI), labels, env, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H2vWbrQ4BH-",
        "outputId": "e1da3df7-cf1f-46b9-c1a7-85974fac8da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER POLICY ITERATION\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.403 0.469 0.552 0.480]\n",
            " [0.472 0.000 0.637 0.000]\n",
            " [0.555 0.654 0.751 0.000]\n",
            " [0.000 0.737 0.867 1.000]]\n",
            "\n",
            "Iterations to converge: 2\n",
            "(theta=0.001)\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER VALUE ITERATION\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.455 0.504 0.579 0.505]\n",
            " [0.508 0.000 0.653 0.000]\n",
            " [0.584 0.672 0.768 0.000]\n",
            " [0.000 0.771 0.887 1.000]]\n",
            "\n",
            "Iterations to converge: 10\n",
            "(theta=0.001)\n"
          ]
        }
      ]
    }
  ]
}