{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 2:<br>**Tabular Model-based Methods**"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# Saving the present working directory's path:\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "d66760d4-d5ef-4b7d-c188-fd9b239cb539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "import numpy as np\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "b305ef02-00a0-4ba3-f17c-f932cc9eee08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import contextlib\n",
        "\n",
        "# Configures numpy print options\n",
        "@contextlib.contextmanager\n",
        "def _printoptions(*args, **kwargs):\n",
        "    original = np.get_printoptions()\n",
        "    np.set_printoptions(*args, **kwargs)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        np.set_printoptions(**original)"
      ],
      "metadata": {
        "id": "VrK-GtTF_5m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy evaluation"
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma, theta, max_iterations):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    `policy`: Array giving the probability of taking an action from a state\n",
        "    `gamma`: Discount factor\n",
        "    `theta`: Error tolerance level\n",
        "\n",
        "    FURTHER NOTE ON `policy`:\n",
        "    We consider the policy to be deterministic, meaning that it maps each state\n",
        "    to a certain action, rather than each state-action pair to a probability.\n",
        "    Hence, `policy` is a 1D array with each index corresponding to a state and\n",
        "    the value at a given index i corresponding to the action to be taken from\n",
        "    state i. This is equivalent to the policy wherein each state-action pair is\n",
        "    related to either 0 or 1 so that each state is mapped to 1 only when paired\n",
        "    with a particular action.\n",
        "    '''\n",
        "    # Initialising table of values per state:\n",
        "    value = np.zeros(env.n_states, dtype=np.float)\n",
        "    # Flag for indicating convergence of value evaluation:\n",
        "    flag = 0\n",
        "    # Policy evaluation loop:\n",
        "    for i in range(max_iterations):\n",
        "        for s in range(env.n_states):\n",
        "            # NOTE: s ==> state\n",
        "            # Storing previous value of value function:\n",
        "            v = value[s]\n",
        "            #------------------------------------\n",
        "            # Obtaining current value of value function:\n",
        "            # NOTE: We iterate through every possible action from state `s`\n",
        "            value[s] = 0\n",
        "            for a in range(env.n_actions):\n",
        "                # NOTE: a ==> action\n",
        "                # If policy does not map s to a, move to next action:\n",
        "                if policy[s] != a: continue\n",
        "                # If policy does map s to a, update state value:\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    value[s] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    env.p: Probability of moving from s to _s given action a\n",
        "                    env.r: Reward of moving from s to _s given action a\n",
        "                    '''\n",
        "            #------------------------------------\n",
        "            # Obtaining the difference in state value:\n",
        "            # NOTE: This is why we stored the previous value before in `v`\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "    return value"
      ],
      "metadata": {
        "id": "UedO36FhIPP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy improvement"
      ],
      "metadata": {
        "id": "roJtJVb5d87b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(env, value, gamma):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    `value`: Array containing values of each state with respect to some policy\n",
        "    `gamma`: Discount factor\n",
        "\n",
        "    NOTE ON POLICY IMPROVEMENT:\n",
        "    The goal of policy improvement is to improve on the previously used policy\n",
        "    (which is implicit in the array of state values, which is evaluated with\n",
        "    respect to some policy). We do this by choosing for each state s the action\n",
        "    a such that we maximise the reward of taking a from s (irrespective of\n",
        "    policy) then following the previous policy (which is implicit in the array\n",
        "    of state values).\n",
        "    '''\n",
        "    policy = np.zeros(env.n_states, dtype=int)\n",
        "    for s in range(env.n_states):\n",
        "        q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "        for a in range(env.n_actions):\n",
        "            for _s in range(env.n_states):\n",
        "                # NOTE: _s ==> next state\n",
        "                # Total reward of taking a from s then following last policy:\n",
        "                '''\n",
        "                NOTE ON LAST POLICY:\n",
        "                The previous policy based on which we are making the current\n",
        "                improvement is implicit in the array of state values `value`,\n",
        "                since this array was obtained with respect to some policy.\n",
        "                '''\n",
        "                q[a] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                '''\n",
        "                NOTE ON ABOVE USED FUNCTIONS:\n",
        "                env.p: Probability of moving from s to _s given action a\n",
        "                env.r: Reward of moving from s to _s given action a\n",
        "                '''\n",
        "        # Update policy to maximise the one-step dynamics from s:\n",
        "        policy[s] = np.argmax(q)\n",
        "    return policy"
      ],
      "metadata": {
        "id": "A4D_aCb4dvhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy iteration..."
      ],
      "metadata": {
        "id": "gO_Ul57neABk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma, theta, max_iterations, policy=None):\n",
        "    if policy is None: policy = np.zeros(env.n_states, dtype=int)\n",
        "    else: policy = np.array(policy, dtype=int)\n",
        "    # Initialising state values with respect to existing policy:\n",
        "    value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "    # Policy iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        policy = policy_improvement(env, value, gamma)\n",
        "        new_value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "        # If all value evaluations change less than theta, break:\n",
        "        if all(abs(new_value-value) < theta): break\n",
        "        # Else, continue improving with the newly evaluated state values:\n",
        "        value = new_value\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "kWqlg13zWAxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value iteration"
      ],
      "metadata": {
        "id": "w4oRY11heB7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma, theta, max_iterations, value=None):\n",
        "    if value is None: value = np.zeros(env.n_states)\n",
        "    else: value = np.array(value, dtype=np.float)\n",
        "    # Flag for indicating convergence of value evaluation:\n",
        "    flag = 0\n",
        "    # Value iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        for s in range(env.n_states):\n",
        "            q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "            for a in range(env.n_actions):\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    # Total reward of taking a from s for previous state value:\n",
        "                    q[a] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    env.p: Probability of moving from s to _s given action a\n",
        "                    env.r: Reward of moving from s to _s given action a\n",
        "                    '''\n",
        "            # Update policy to maximise the one-step dynamics from s:\n",
        "            v = value[s]\n",
        "            value[s] = np.max(q)\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the (estimated) optimal policy:\n",
        "    # NOTE: The logic for this is identical to policy improvement\n",
        "    policy = policy_improvement(env, value, gamma)\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "zOCfwm_0d166"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
