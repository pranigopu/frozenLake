{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 5:<br>**Deep Reinforcement Learning**\n",
        "\n",
        "## Introduction to the concept\n",
        "Suppose the states could be represented as vectors of features, and suppose the action-reward function (that enables us to obtain estimated optimal policies) can be approximated using a non-linear function of the aforementioned features. Under these assumptions, we use convolutional neural networks to help (1) distill the features of a state to essential features, (2) approximate action-rewards using the distilled essential features."
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "# Saving the present working directory's path:\n",
        "# NOTE: Change `pwd` based on your own Google Drive organisation\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "bc2b18c2-5755-4a29-91a9-37c3b921e66e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "525fb322-8577-4f07-ad0c-7e91771d1d75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "Eqid0ZW_qvkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "xEpt9SkPotQo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the environment to enable feature mapping\n",
        "State image is composed of four channels and is represented by a `numpy.array` of shape $(4, h, w)$, where $h$ is the number of rows and $w$ is the number of columns of the lake grid.\n",
        "\n",
        "### DEMO: State image representation to be used"
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing lines for neat presentation:\n",
        "div1 = '\\n================================================\\n'\n",
        "div2 = '------------------------------------'\n",
        "\n",
        "# Printing the original grid:\n",
        "myLake = np.array(lake['small'])\n",
        "print(f'The original frozen lake grid:\\n{myLake}\\n{div1}')\n",
        "\n",
        "# Printing channels 2, 3 & 4 for each state image:\n",
        "lake_image = [(np.array(myLake) == c).astype(float) for c in ['&', '#', '$']]\n",
        "print('Channels 2, 3 & 4 for each state image')\n",
        "L = ['C2. Start tile marker', 'C3. Hole tile marker', 'C4. Goal tile marker']\n",
        "for l, A in zip(L, lake_image): print(f'{div2}\\n{l}:\\n{A}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYlSRqMdgBpa",
        "outputId": "679a9aad-f30f-45b2-f8d6-3de9a7f06094"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original frozen lake grid:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "\n",
            "================================================\n",
            "\n",
            "Channels 2, 3 & 4 for each state image\n",
            "------------------------------------\n",
            "C2. Start tile marker:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "------------------------------------\n",
            "C3. Hole tile marker:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 1. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]]\n",
            "------------------------------------\n",
            "C4. Goal tile marker:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing wrapper class"
      ],
      "metadata": {
        "id": "ImxvgS6BlUcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenLakeImageWrapper:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        lake = self.env.lake\n",
        "        # NOTE: The lake grid is converted into an array by the environment\n",
        "        self.n_actions = self.env.n_actions\n",
        "\n",
        "        # Obtaining a state image for each state:\n",
        "        #------------------------------------\n",
        "        # 1. Shape for each state image:\n",
        "        self.state_shape = (4, lake.shape[0], lake.shape[1])\n",
        "        #------------------------------------\n",
        "        # 2. Obtaining a list of filter arrays:\n",
        "        lake_image = [(lake == c).astype(float) for c in ['&', '#', '$']]\n",
        "        #------------------------------------\n",
        "        # 3. Obtaining the state image for each state:\n",
        "        #........................\n",
        "        # Handling for the absorbing state...\n",
        "        # a. Channel 1 of the state:\n",
        "        # NOTE: Absorbing state has no position on the grid, so all zeros\n",
        "        A = np.zeros(lake.shape)\n",
        "\n",
        "        # b. Attaching channels 2, 3 & 4, then storing all as an array:\n",
        "        self.state_image = {env.absorbing_state: np.stack([A] + lake_image)}\n",
        "        '''\n",
        "        IMPLEMENTATION NOTE:\n",
        "        `[A]` is a list containing array A, and `lake_image` is a list\n",
        "        containing 3 arrays. Using `+` between `[A]` and `lake_image` will\n",
        "        concatenate the two lists, resulting in a list of 4 arrays.\n",
        "\n",
        "        `np.stack` joins the above array list into a single array of arrays.\n",
        "        '''\n",
        "        #........................\n",
        "        # Handling for the other states actually present on the grid...\n",
        "        for state in range(lake.size):\n",
        "            # a. Channel 1 of the state:\n",
        "            '''\n",
        "            NOTE ON CHANNEL 1:\n",
        "            The 1st channel is the array such that the element is 1 if the\n",
        "            index matches the state, 0 otherwise. This corresponds to the\n",
        "            position of the agent if the agent were to be in this state. Hence,\n",
        "            note that the 1st channel shows not the current position of the\n",
        "            agent, but its position if it were in this state.\n",
        "            '''\n",
        "            # a.1. Initialising it as an array of zeros:\n",
        "            A = np.zeros(lake.shape)\n",
        "            # a.2. Assigning the current state's position as 1:\n",
        "            row = state // lake.shape[0]\n",
        "            col = state % lake.shape[1]\n",
        "            A[row, col] = 1.0\n",
        "\n",
        "            # b. Attaching channels 2, 3 & 4, then storing all as an array:\n",
        "            self.state_image[state] = np.stack([A] + lake_image)\n",
        "            '''\n",
        "            IMPLEMENTATION NOTE:\n",
        "            Check the implementation note above this loop.\n",
        "            '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Mapping the given state paired with each action to state image:\n",
        "    # NOTE: State images were obtained for each state in the class constructor\n",
        "\n",
        "    def encode_state(self, state):\n",
        "        return self.state_image[state]\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the policy via decoding neural network's output:\n",
        "    # 1. Encode states as state images\n",
        "    # 2. Pass state images as input to the neural network\n",
        "    # 3. Obtain the action-value function as an output\n",
        "    # 4. Use the action-value function to obtain the policy & state-values\n",
        "\n",
        "    def decode_policy(self, dqn):\n",
        "        # 1. Encode states as state images:\n",
        "        N = self.env.n_states\n",
        "        states = np.array([self.encode_state(s) for s in range(N)])\n",
        "\n",
        "        # 2 & 3: Obtain the action-value function for encoded states:\n",
        "        q = dqn(states).detach().numpy()\n",
        "        # NOTE: `torch.no_grad` omitted to avoid import\n",
        "\n",
        "        # 4. Use the action-value function to obtain the policy & state-values:\n",
        "        policy = q.argmax(axis=1)\n",
        "        value = q.max(axis=1)\n",
        "        return policy, value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Resetting environment & encoding it as state image:\n",
        "\n",
        "    def reset(self):\n",
        "        return self.encode_state(self.env.reset())\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Taking a step in environment & encoding next state as state image:\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done = self.env.step(action)\n",
        "        return self.encode_state(state), reward, done\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Visualising the agent's performance (by inputs or using a policy):\n",
        "\n",
        "    def render(self, policy=None, value=None):\n",
        "        self.env.render(policy, value)"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the neural network"
      ],
      "metadata": {
        "id": "0Q32IoP4WOYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetwork(torch.nn.Module):\n",
        "    def __init__(self, wenv, learning_rate, kernel_size,\n",
        "                 conv_out_channels, fc_out_features, seed):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Convolutional layer:\n",
        "        self.conv_layer = torch.nn.Conv2d(in_channels=wenv.state_shape[0],\n",
        "                                          out_channels=conv_out_channels,\n",
        "                                          kernel_size=kernel_size, stride=1)\n",
        "\n",
        "        # h ==> Number of rows in grid, w ==> Number of columns in grid\n",
        "        h = wenv.state_shape[1] - kernel_size + 1\n",
        "        w = wenv.state_shape[2] - kernel_size + 1\n",
        "\n",
        "        # Fully connected layer:\n",
        "        self.fc_layer = torch.nn.Linear(in_features=h*w*conv_out_channels,\n",
        "                                        out_features=fc_out_features)\n",
        "\n",
        "        # Output layer:\n",
        "        self.output_layer = torch.nn.Linear(in_features=fc_out_features,\n",
        "                                            out_features=wenv.n_actions)\n",
        "\n",
        "        # Optimiser for gradient descent:\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Feed-forward function:\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Setting the activation function:\n",
        "        activation = torch.nn.ReLU()\n",
        "\n",
        "        #print(f'0.1: x: {x}')\n",
        "        y = torch.tensor(x, dtype=torch.float)\n",
        "        #print(f'0.2: y: {y}')\n",
        "        '''\n",
        "        EXPECTED SHAPE OF THE ABOVE INPUT TENSOR:\n",
        "        `x.shape` = (B, 4, h, w), where\n",
        "        >> B ==> Number of states\n",
        "        >> 4 ==> Number of channels per state representation\n",
        "        >> h ==> Number of rows in the playing grid\n",
        "        >> w ==> Number of columns in the playing grid\n",
        "        '''\n",
        "        # Feeding forward the input across layers:\n",
        "        y = self.conv_layer(y)\n",
        "        #print(f'1: y: {y}')\n",
        "        y = activation(y)\n",
        "        #print(f'2: y: {y}')\n",
        "\n",
        "        # Flattening `x` before passing it to the fully connected layer:\n",
        "        y = torch.flatten(y, start_dim=1)\n",
        "        #print(f'3: y: {y}')\n",
        "        '''\n",
        "        NOTE ON FLATTENING:\n",
        "        We want to flatten each state image representation. Now, a state image\n",
        "        consists of `conv_out_channels` channels, each of shape `(h, w)`.\n",
        "        Hence, each state image needs to become a `h*w*conv_out_channels` sized\n",
        "        tensor.\n",
        "\n",
        "        Now, `x` holds B states, and before applying ReLU (which does not alter\n",
        "        the input tensor shape), the state images are arranged in `x` such\n",
        "        that `x` was an array of arrays, each array being a state image, which\n",
        "        means the 1st dimension of `x` corresponds to the states. This means\n",
        "        we want to flatten each state representation while maintaining the\n",
        "        an array of state representations. Hence, we leave the 1st dimension\n",
        "        of `x` (i.e. axis 0) and start flattening from the 2nd dimension\n",
        "        (i.e. axis 1, leading to the argument `start_dim=1`).\n",
        "        '''\n",
        "\n",
        "        y = self.fc_layer(y)\n",
        "        #print(f'4.1: y: {y}')\n",
        "        y = activation(y)\n",
        "        #print(f'4.2: y: {y}')\n",
        "\n",
        "        y = self.output_layer(y)\n",
        "        #print(f'5: y: {y}')\n",
        "        return y\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Single step of training:\n",
        "\n",
        "    def train_step(self, transitions, gamma, tdqn, episode):\n",
        "        # Organising the transitions data into separate arrays:\n",
        "        states = np.array([transition[0] for transition in transitions])\n",
        "        actions = np.array([transition[1] for transition in transitions])\n",
        "        rewards = np.array([transition[2] for transition in transitions])\n",
        "        next_states = np.array([transition[3] for transition in transitions])\n",
        "        dones = np.array([transition[4] for transition in transitions])\n",
        "        #print(f\"Transitions:\\nstates: {states}\\nactions: {actions}\\nrewards: {rewards}\\nnext_states: {next_states}\\ndones: {dones}\\n\\n\")\n",
        "\n",
        "        # Obtaining current action-value estimates:\n",
        "        q = self(states)\n",
        "        # NOTE: The above is equivalent to doing `q = self.forward(states)`\n",
        "\n",
        "        #print(f'ORIGINAL:\\nepisode: {episode}\\nr: {rewards}\\nq: {q}')\n",
        "        q = q.gather(1, torch.Tensor(actions).view(len(transitions), 1).long())\n",
        "        #print(f'A:\\nq: {q}')\n",
        "        q = q.view(len(transitions))\n",
        "        #print(f'B\\nq: {q}\\n\\n')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = tdqn(next_states).max(dim=1)[0] * (1 - dones)\n",
        "            '''\n",
        "            EXPLAINING THE ABOVE LINE:\n",
        "            `tdqn(next_states)` is equivalent to `tdqn.forward(next_states)`,\n",
        "            and simply applies the forward model of the non-updated model\n",
        "            (stored in `tdqn`) to the next states, to get an estimate of\n",
        "            action-values given the previous weights of the model.\n",
        "            ------------------------------------\n",
        "            The `.max` function, when applied to a tensor, produces two tensors:\n",
        "            1. The array of max value(s) along the specified dimension\n",
        "            2. The dimension-specific indices where the max value(s) were found\n",
        "\n",
        "            We only want the first of the above two tensors. Hence, we apply\n",
        "            the subscript `[0]` on `tdqn(next_states).max(dim=1)`, to do\n",
        "            `tdqn(next_states).max(dim=1)[0]`\n",
        "            '''\n",
        "\n",
        "        #print(f'HELLO:{tdqn(next_states)}\\n\\n')\n",
        "\n",
        "        # Estimating the one-step rewards given the stored rewards:\n",
        "        target = torch.Tensor(rewards) + gamma*next_q\n",
        "\n",
        "        # Loss calculation:\n",
        "        # NOTE 1: The loss is the mean squared error between `q` & `target`\n",
        "        # NOTE 2: `q - target` is temporal difference for given state-action\n",
        "        #loss = torch.mean((q - target)**2)\n",
        "        loss = torch.nn.functional.mse_loss(q, target.to(torch.float32))\n",
        "\n",
        "        # Performing gradient descent, i.e. optimisation:\n",
        "        self.optimizer.zero_grad() # Intialising gradient as zero\n",
        "        loss.backward()            # Computing the current gradient\n",
        "        self.optimizer.step()      # Performing the optimisation step"
      ],
      "metadata": {
        "id": "he0rko6UWKKf"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class `ReplayBuffer` implements a replay buffer that stores transitions. A transition is a tuple composed of a state, action, reward, next state, and a flag variable that denotes whether the episode ended at the next state. The buffer is represented by a Python deque object that automatically discards the oldest transitions when it reaches capacity. The method `draw` returns a list of $n$ transitions ($n \\implies$ batch size) drawn without replacement from the replay buffer."
      ],
      "metadata": {
        "id": "PSaSnHfl7D1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, random_state):\n",
        "        # Replay buffer data structure:\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "        # Maintaining the given random state for enabling replicability:\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def draw(self, batch_size):\n",
        "        # Length of the replay buffer:\n",
        "        N = self.__len__()\n",
        "\n",
        "        # Randomly sampling `batch_size` buffer indices without replacement:\n",
        "        I = self.random_state.choice(N, size=batch_size, replace=False)\n",
        "\n",
        "        # Returning the transitions corresponding to the above indices:\n",
        "        return [self.buffer[i] for i in I]"
      ],
      "metadata": {
        "id": "F7orM04wsEqi"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning process"
      ],
      "metadata": {
        "id": "3tGVyyURHWee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_q_network_learning(env, max_episodes, lr,\n",
        "                            gamma, epsilon, batch_size,\n",
        "                            target_update_frequency, buffer_size, kernel_size,\n",
        "                            conv_out_channels, fc_out_features, seed):\n",
        "    # INITIALISATION\n",
        "    # Setting random state with given seed for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising replay buffer\n",
        "    replay_buffer = ReplayBuffer(buffer_size, random_state)\n",
        "\n",
        "    # Initialising the required deep neural networks:\n",
        "    args = [env, lr, kernel_size, conv_out_channels, fc_out_features, seed]\n",
        "    dqn = DeepQNetwork(*args)\n",
        "    tdqn = DeepQNetwork(*args)\n",
        "\n",
        "    # Array of linearly decreasing exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Choosing next action with epsilon-greedy policy:\n",
        "            if random_state.rand() < epsilon[i]:\n",
        "                action = random_state.choice(env.n_actions)\n",
        "            else:\n",
        "                with torch.no_grad(): q = dqn(np.array([state]))[0].numpy()\n",
        "                qmax = np.max(q)\n",
        "                best = [a for a in range(env.n_actions) if np.allclose(qmax, q[a])]\n",
        "                action = random_state.choice(best)\n",
        "\n",
        "            # Moving the agent to the next state within the current episode:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            # Updating the replay buffer:\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            # Updating the state variable:\n",
        "            state = next_state\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                transitions = replay_buffer.draw(batch_size)\n",
        "                dqn.train_step(transitions, gamma, tdqn, i)\n",
        "\n",
        "        if (i % target_update_frequency) == 0:\n",
        "            tdqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "zkWyTmO9uHKv"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "hTiJ0vYqNwqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake['small'], 0.1, 100)\n",
        "    wenv = FrozenLakeImageWrapper(env)\n",
        "    max_episodes = 1000\n",
        "    lr = 0.01 # Learning rate\n",
        "    gamma = 0.9\n",
        "    epsilon = 0.5\n",
        "    batch_size = 50\n",
        "    target_update_frequency = 1\n",
        "    buffer_size = 500\n",
        "    kernel_size = 4\n",
        "    conv_out_channels = 12\n",
        "    fc_out_features = 12\n",
        "    seed = 0\n",
        "\n",
        "    # Running the function:\n",
        "    DeepQ = deep_q_network_learning(wenv,\n",
        "                                    max_episodes,\n",
        "                                    lr,\n",
        "                                    gamma, epsilon,\n",
        "                                    batch_size,\n",
        "                                    target_update_frequency,\n",
        "                                    buffer_size,\n",
        "                                    kernel_size,\n",
        "                                    conv_out_channels,\n",
        "                                    fc_out_features,\n",
        "                                    seed)\n",
        "\n",
        "    # Obtaining the policy & state values:\n",
        "    DeepQ = wenv.decode_policy(DeepQ)\n",
        "    labels = (\"deep q network learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults([DeepQ], labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhM4W_BNx4a",
        "outputId": "d5e213b8-1e22-43a9-9321-a6878d20293d"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER D\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '^' '_' '<']\n",
            " ['_' '<' '_' '_']\n",
            " ['>' '>' '_' '_']\n",
            " ['_' '_' '>' '<']]\n",
            "Value:\n",
            "[[0.441 0.500 0.568 0.486]\n",
            " [0.516 0.030 0.654 0.047]\n",
            " [0.515 0.540 0.754 0.027]\n",
            " [0.013 0.664 0.886 1.020]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE ON SETTING LEARNING RATE**:<br>If the learning rate is set too high, the gradient descent process will tend to overshoot the optimum. The result of this in our case is that the weights become highly negative overall, producing results such that applying ReLU leads to a zero-matrix. This zero-matrix leads to every resultant row of the final output (i.e. the action-values for each state) being equal, leading to a situation where:\n",
        "\n",
        "- The forward model produces the same action-values for each action, no matter the state\n",
        "- As a result of the above, the same maximum action-value is indicated for each state\n",
        "- As a result of the above, action-values & thus policy converge to the same value & same action for each state\n",
        "\n",
        "Hence, set the learning rate sufficiently low to prevent such an overshooting gradient descent."
      ],
      "metadata": {
        "id": "SDG3xkt0uK_y"
      }
    }
  ]
}
