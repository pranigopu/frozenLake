{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 5:<br>**Deep Reinforcement Learning**\n",
        "\n",
        "## Introduction to the concept\n",
        "Suppose the states could be represented as vectors of features, and suppose the action-reward function (that enables us to obtain estimated optimal policies) can be approximated using a non-linear function of the aforementioned features. Under these assumptions, we use convolutional neural networks to help (1) distill the features of a state to essential features, (2) approximate action-rewards using the distilled essential features."
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Mounting the Google Drive folder (run if necessary):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    # Saving the present working directory's path:\n",
        "    # NOTE: Change `pwd` based on your own Google Drive organisation\n",
        "    pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "ce726953-0c59-4d6f-dcbf-8adad56d8bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import import_ipynb\n",
        "    N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "    N.load_module(\"Q1_environment\")\n",
        "    from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "5c2832fa-d2d2-4003-ee60-572a24d7b0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "Eqid0ZW_qvkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "xEpt9SkPotQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the environment to enable feature mapping\n",
        "State image is composed of four channels and is represented by a `numpy.array` of shape $(4, h, w)$, where $h$ is the number of rows and $w$ is the number of columns of the lake grid.\n",
        "\n",
        "### DEMO: State image representation to be used"
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Dividing lines for neat presentation:\n",
        "    div1 = '\\n================================================\\n'\n",
        "    div2 = '------------------------------------'\n",
        "\n",
        "    # Printing the original grid:\n",
        "    myLake = np.array(lake['small'])\n",
        "    print(f'The original frozen lake grid:\\n{myLake}\\n{div1}')\n",
        "\n",
        "    # Printing channels 2, 3 & 4 for each state image:\n",
        "    lake_image = [(np.array(myLake) == c).astype(float) for c in ['&', '#', '$']]\n",
        "    print('Channels 2, 3 & 4 for each state image')\n",
        "    L = ['C2. Start tile marker', 'C3. Hole tile marker', 'C4. Goal tile marker']\n",
        "    for l, A in zip(L, lake_image): print(f'{div2}\\n{l}:\\n{A}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYlSRqMdgBpa",
        "outputId": "44adb104-3a13-4525-ce7a-e9370bbd8f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original frozen lake grid:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "\n",
            "================================================\n",
            "\n",
            "Channels 2, 3 & 4 for each state image\n",
            "------------------------------------\n",
            "C2. Start tile marker:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "------------------------------------\n",
            "C3. Hole tile marker:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 1. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]]\n",
            "------------------------------------\n",
            "C4. Goal tile marker:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrapping the environment to enable conversion of states to images"
      ],
      "metadata": {
        "id": "ImxvgS6BlUcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenLakeImageWrapper:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        lake = self.env.lake\n",
        "        # NOTE: The lake grid is converted into an array by the environment\n",
        "        self.n_actions = self.env.n_actions\n",
        "\n",
        "        # Obtaining a state image for each state:\n",
        "        #------------------------------------\n",
        "        # 1. Shape for each state image:\n",
        "        self.state_shape = (4, lake.shape[0], lake.shape[1])\n",
        "        #------------------------------------\n",
        "        # 2. Obtaining a list of filter arrays:\n",
        "        lake_image = [(lake == c).astype(float) for c in ['&', '#', '$']]\n",
        "        #------------------------------------\n",
        "        # 3. Obtaining the state image for each state:\n",
        "        #........................\n",
        "        # Handling for the absorbing state...\n",
        "        # a. Channel 1 of the state:\n",
        "        # NOTE: Absorbing state has no position on the grid, so all zeros\n",
        "        A = np.zeros(lake.shape)\n",
        "\n",
        "        # b. Attaching channels 2, 3 & 4, then storing all as an array:\n",
        "        self.state_image = {env.absorbing_state: np.stack([A] + lake_image)}\n",
        "        '''\n",
        "        IMPLEMENTATION NOTE:\n",
        "        `[A]` is a list containing array A, and `lake_image` is a list\n",
        "        containing 3 arrays. Using `+` between `[A]` and `lake_image` will\n",
        "        concatenate the two lists, resulting in a list of 4 arrays.\n",
        "\n",
        "        `np.stack` joins the above array list into a single array of arrays.\n",
        "        '''\n",
        "        #........................\n",
        "        # Handling for the other states actually present on the grid...\n",
        "        for state in range(lake.size):\n",
        "            # a. Channel 1 of the state:\n",
        "            '''\n",
        "            NOTE ON CHANNEL 1:\n",
        "            The 1st channel is the array such that the element is 1 if the\n",
        "            index matches the state, 0 otherwise. This corresponds to the\n",
        "            position of the agent if the agent were to be in this state. Hence,\n",
        "            note that the 1st channel shows not the current position of the\n",
        "            agent, but its position if it were in this state.\n",
        "            '''\n",
        "            # a.1. Initialising it as an array of zeros:\n",
        "            A = np.zeros(lake.shape)\n",
        "            # a.2. Assigning the current state's position as 1:\n",
        "            row = state // lake.shape[0]\n",
        "            col = state % lake.shape[1]\n",
        "            A[row, col] = 1.0\n",
        "\n",
        "            # b. Attaching channels 2, 3 & 4, then storing all as an array:\n",
        "            self.state_image[state] = np.stack([A] + lake_image)\n",
        "            '''\n",
        "            IMPLEMENTATION NOTE:\n",
        "            Check the implementation note above this loop.\n",
        "            '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Mapping the given state paired with each action to state image:\n",
        "    # NOTE: State images were obtained for each state in the class constructor\n",
        "\n",
        "    def encode_state(self, state):\n",
        "        return self.state_image[state]\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the policy via decoding neural network's output:\n",
        "    # 1. Encode states as state images\n",
        "    # 2. Pass state images as input to the neural network\n",
        "    # 3. Obtain the action-value function as an output\n",
        "    # 4. Use the action-value function to obtain the policy & state-values\n",
        "\n",
        "    def decode_policy(self, dqn):\n",
        "        # 1. Encode states as state images:\n",
        "        N = self.env.n_states\n",
        "        states = np.array([self.encode_state(s) for s in range(N)])\n",
        "\n",
        "        # 2 & 3: Obtain the action-value function for encoded states:\n",
        "        q = dqn(states).detach().numpy()\n",
        "        # NOTE: `torch.no_grad` omitted to avoid import\n",
        "\n",
        "        # 4. Use the action-value function to obtain the policy & state-values:\n",
        "        policy = q.argmax(axis=1)\n",
        "        value = q.max(axis=1)\n",
        "        return policy, value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Resetting environment & encoding it as state image:\n",
        "\n",
        "    def reset(self):\n",
        "        return self.encode_state(self.env.reset())\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Taking a step in environment & encoding next state as state image:\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done = self.env.step(action)\n",
        "        return self.encode_state(state), reward, done\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Visualising the agent's performance (by inputs or using a policy):\n",
        "\n",
        "    def render(self, policy=None, value=None):\n",
        "        self.env.render(policy, value)"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network implementation"
      ],
      "metadata": {
        "id": "0Q32IoP4WOYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetwork(torch.nn.Module):\n",
        "    def __init__(self, wenv, learning_rate, kernel_size,\n",
        "                 conv_out_channels, fc_out_features, seed):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Convolutional layer:\n",
        "        self.conv_layer = torch.nn.Conv2d(in_channels=wenv.state_shape[0],\n",
        "                                          out_channels=conv_out_channels,\n",
        "                                          kernel_size=kernel_size, stride=1)\n",
        "\n",
        "        # h ==> Number of rows in grid, w ==> Number of columns in grid\n",
        "        h = wenv.state_shape[1] - kernel_size + 1\n",
        "        w = wenv.state_shape[2] - kernel_size + 1\n",
        "\n",
        "        # Fully connected layer:\n",
        "        self.fc_layer = torch.nn.Linear(in_features=h*w*conv_out_channels,\n",
        "                                        out_features=fc_out_features)\n",
        "\n",
        "        # Output layer:\n",
        "        self.output_layer = torch.nn.Linear(in_features=fc_out_features,\n",
        "                                            out_features=wenv.n_actions)\n",
        "\n",
        "        # Optimiser for gradient descent:\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Feed-forward function:\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Setting the activation function:\n",
        "        activation = torch.nn.ReLU()\n",
        "\n",
        "        # Converting inputted array into a tensor:\n",
        "        y = torch.tensor(x, dtype=torch.float)\n",
        "        '''\n",
        "        EXPECTED SHAPE OF THE ABOVE INPUT ARRAY / TENSOR:\n",
        "        `x.shape` = `y.shape` = (B, 4, h, w), where\n",
        "        >> B ==> Number of states\n",
        "        >> 4 ==> Number of channels per state representation\n",
        "        >> h ==> Number of rows in the playing grid\n",
        "        >> w ==> Number of columns in the playing grid\n",
        "        '''\n",
        "        # Feeding forward the input to convolution layer:\n",
        "        y = self.conv_layer(y)\n",
        "        y = activation(y)\n",
        "\n",
        "        # Flattening `x` before passing it to the fully connected layer:\n",
        "        y = torch.flatten(y, start_dim=1)\n",
        "        '''\n",
        "        NOTE ON FLATTENING:\n",
        "        We want to flatten each state image representation. Now, a state image\n",
        "        consists of `conv_out_channels` channels, each of shape `(h, w)`.\n",
        "        Hence, each state image needs to become a `h*w*conv_out_channels` sized\n",
        "        tensor.\n",
        "\n",
        "        Now, `x` holds B states, and before applying ReLU (which does not alter\n",
        "        the input tensor shape), the state images are arranged in `x` such\n",
        "        that `x` was an array of arrays, each array being a state image, which\n",
        "        means the 1st dimension of `x` corresponds to the states. This means\n",
        "        we want to flatten each state representation while maintaining the\n",
        "        an array of state representations. Hence, we leave the 1st dimension\n",
        "        of `x` (i.e. axis 0) and start flattening from the 2nd dimension\n",
        "        (i.e. axis 1, leading to the argument `start_dim=1`).\n",
        "        '''\n",
        "\n",
        "        # Feeding forward the input to fully-connected layer:\n",
        "        y = self.fc_layer(y)\n",
        "        y = activation(y)\n",
        "\n",
        "        # Feeding forward the input to output layer & returning output:\n",
        "        y = self.output_layer(y)\n",
        "        return y\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Single step of training:\n",
        "\n",
        "    def train_step(self, transitions, gamma, tdqn):\n",
        "        # TDQN ==> Target deep Q-network\n",
        "        # ... explained in comments for function `deep_q_network_learning`\n",
        "\n",
        "        # Organising the transitions data into separate arrays:\n",
        "        states = np.array([transition[0] for transition in transitions])\n",
        "        actions = np.array([transition[1] for transition in transitions])\n",
        "        rewards = np.array([transition[2] for transition in transitions])\n",
        "        next_states = np.array([transition[3] for transition in transitions])\n",
        "        dones = np.array([transition[4] for transition in transitions])\n",
        "\n",
        "        # Obtaining current action-value estimates:\n",
        "        q = self(states)\n",
        "        # NOTE: The above is equivalent to doing `q = self.forward(states)`\n",
        "\n",
        "        # Obtaining action-values for previously taken actions:\n",
        "        q = q.gather(1, torch.Tensor(actions).view(len(transitions), 1).long())\n",
        "        q = q.view(len(transitions))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = tdqn(next_states).max(dim=1)[0] * (1 - dones)\n",
        "            '''\n",
        "            EXPLAINING THE ABOVE LINE:\n",
        "            `tdqn(next_states)` is equivalent to `tdqn.forward(next_states)`,\n",
        "            and simply applies the forward model of the non-updated model\n",
        "            (stored in `tdqn`) to the next states, to get an estimate of\n",
        "            action-values given the previous weights of the model.\n",
        "            ------------------------------------\n",
        "            The `.max` function, when applied to a tensor, produces two tensors:\n",
        "            1. The array of max value(s) along the specified dimension\n",
        "            2. The dimension-specific indices where the max value(s) were found\n",
        "\n",
        "            We only want the first of the above two tensors. Hence, we apply\n",
        "            the subscript `[0]` on `tdqn(next_states).max(dim=1)`, to do\n",
        "            `tdqn(next_states).max(dim=1)[0]`\n",
        "            '''\n",
        "\n",
        "        # Estimating the one-step rewards given the stored rewards:\n",
        "        target = torch.Tensor(rewards) + gamma*next_q\n",
        "\n",
        "        # Loss calculation:\n",
        "        # NOTE 1: The loss is the mean squared error between `q` & `target`\n",
        "        # NOTE 2: `q - target` is temporal difference for given state-action\n",
        "        loss = torch.nn.functional.mse_loss(q, target.to(torch.float32))\n",
        "        # ALTERNATIVE: `loss = torch.mean((q - target)**2)`\n",
        "\n",
        "        # Performing gradient descent, i.e. optimisation:\n",
        "        self.optimizer.zero_grad() # Intialising gradient as zero\n",
        "        loss.backward()            # Computing the current gradient\n",
        "        self.optimizer.step()      # Performing the optimisation step"
      ],
      "metadata": {
        "id": "he0rko6UWKKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay buffer implementation\n",
        "The following class `ReplayBuffer` implements a replay buffer that stores transitions. A transition is a tuple composed of a state, action, reward, next state, and a flag variable that denotes whether the episode ended at the next state. The buffer is represented by a Python deque object that automatically discards the oldest transitions when it reaches capacity. The method `draw` returns a list of $n$ transitions ($n \\implies$ batch size) drawn without replacement from the replay buffer.\n",
        "\n",
        "<br>**NOTE**: The replay buffer is vital in utilising previously obtained state transitions and observed rewards."
      ],
      "metadata": {
        "id": "PSaSnHfl7D1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, random_state):\n",
        "        # Replay buffer data structure:\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "        # Maintaining the given random state for enabling replicability:\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def draw(self, batch_size):\n",
        "        # Length of the replay buffer:\n",
        "        N = self.__len__()\n",
        "\n",
        "        # Randomly sampling `batch_size` buffer indices without replacement:\n",
        "        I = self.random_state.choice(N, size=batch_size, replace=False)\n",
        "\n",
        "        # Returning the transitions corresponding to the above indices:\n",
        "        return [self.buffer[i] for i in I]"
      ],
      "metadata": {
        "id": "F7orM04wsEqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning process\n",
        "Learning is done by performing gradient descent over the minimum squared error (MSE) loss function. Instead of obtaining loss with respect to differences from observed values alone (observed action-values in our case), we obtain the loss as the mean of the sum of squares of the temporal differences for each state-action pair (_the implementation details are discussed after the following code section_). This is done because rewards are very sparse in the given environment, which means obtaining accurate estimates of action-values from interaction with the environment alone will take an unfeasibly long time, prompting us to instead use temporal differences (_discussed further after the following code section_)."
      ],
      "metadata": {
        "id": "3tGVyyURHWee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_q_network_learning(env, max_episodes, learning_rate,\n",
        "                            gamma, epsilon, batch_size,\n",
        "                            target_update_frequency, buffer_size, kernel_size,\n",
        "                            conv_out_channels, fc_out_features, seed):\n",
        "    # INITIALISATION\n",
        "\n",
        "    # Setting random state with given seed for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising replay buffer\n",
        "    replay_buffer = ReplayBuffer(buffer_size, random_state)\n",
        "\n",
        "    # Initialising the required deep neural networks:\n",
        "    args = [env,\n",
        "            learning_rate,\n",
        "            kernel_size,\n",
        "            conv_out_channels,\n",
        "            fc_out_features,\n",
        "            seed]\n",
        "    dqn = DeepQNetwork(*args)\n",
        "    tdqn = DeepQNetwork(*args)\n",
        "    '''\n",
        "    IMPLEMENTATION & CONCEPTUAL NOTES:\n",
        "    See the text below this code section.\n",
        "    '''\n",
        "\n",
        "    # Array of linearly decreasing exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Choosing next action with epsilon-greedy policy:\n",
        "            if random_state.rand() < epsilon[i]:\n",
        "                action = random_state.choice(env.n_actions)\n",
        "            else:\n",
        "                with torch.no_grad(): q = dqn(np.array([state]))[0].numpy()\n",
        "                qmax = np.max(q)\n",
        "                best = [a for a in range(env.n_actions) if np.allclose(qmax, q[a])]\n",
        "                action = random_state.choice(best)\n",
        "\n",
        "            # Moving the agent to the next state within the current episode:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            # Updating the replay buffer:\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            # Updating the state variable:\n",
        "            state = next_state\n",
        "\n",
        "            # Once we have enough stored in the replay buffer, draw from it:\n",
        "            # NOTE: The drawn data is used to train the network\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                transitions = replay_buffer.draw(batch_size)\n",
        "                dqn.train_step(transitions, gamma, tdqn)\n",
        "\n",
        "        #------------------------------------\n",
        "        # Updating the network weights of `tdqn` as per update frequency:\n",
        "        if (i % target_update_frequency) == 0:\n",
        "            tdqn.load_state_dict(dqn.state_dict())\n",
        "            # NOTE: The above copies the parameter values of `dqn` to `tdqn`\n",
        "\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "zkWyTmO9uHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPLEMENTATION NOTE**: `dqn` **&** `tdqn`:\n",
        "\n",
        "- DQN $\\implies$ Deep Q-Network\n",
        "- TDQN $\\implies$ Target Deep Q-Network\n",
        "\n",
        "<br>`dqn` is meant to be the up-to-date network (i.e. with the updated weights) using which we estimated expected rewards for state-action pairs.\n",
        "<br><br>`tdqn` is meant to be the previous network (i.e. before current or recent updates to the weights) using which we obtain the previous estimated expected rewards for state-action pairs. These previous estimates are used as benchmarks to update the current network using temporal difference, i.e. these previous estimates are used as the \"observed target\" values for training the network. Note that we can set the parameters such that `tdqn` is the same as `dqn`. However, keeping `tdqn` less frequently updated can help avoid getting stuck in a local optimum.\n",
        "\n",
        "---\n",
        "\n",
        "**CONCEPTUAL NOTE**: **Theoretical connection**:\n",
        "<br>Given the following temporal difference:\n",
        "\n",
        "  $r_{t+1} + \\gamma \\max_{\\text{ } \\forall a \\in A}(Q(s_{t+1}, a)) - Q(s_t, a_t)$\n",
        "  \n",
        "  Here:\n",
        "\n",
        "  - $t$: Current time stamp\n",
        "  - $A$: The set of all possible actions\n",
        "  - Actions are chosen based on epsilon-greedy policy\n",
        "  - We take action $a_t$ from state $s_t$\n",
        "  - We take action $a_{t+1}$ from state $s_{t+1}$\n",
        "  - $r_{t+1}$: Observed reward of taking $a_t$ from $s_t$\n",
        "  - $Q(s_t, a_t)$: Estimated reward of taking $a_t$ from $s_t$\n",
        "  - $\\max(Q(s_{t+1}, a))$: Maximum action-reward possible from $s_{t+1}$\n",
        "  - $\\gamma$: Discount factor\n",
        "\n",
        "Connecting the above to the implementation:\n",
        "\n",
        "- $max(Q(s_{t+1}, a))$ is computed using `tdqn`\n",
        "- $Q(s_t, a_t)$ is computed using `dqn`\n",
        "- $a_t, s_t, r_{t+1}$ and $s_{t+1}$ are picked from replay buffer"
      ],
      "metadata": {
        "id": "P1AySF3b3_YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "hTiJ0vYqNwqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake['small'], 0.1, 100)\n",
        "    wenv = FrozenLakeImageWrapper(env)\n",
        "    max_episodes = 1000\n",
        "    learning_rate = 0.01 # Learning rate\n",
        "    gamma = 0.9\n",
        "    epsilon = 0.5\n",
        "    batch_size = 50\n",
        "    target_update_frequency = 1\n",
        "    buffer_size = 500\n",
        "    kernel_size = 4\n",
        "    conv_out_channels = 12\n",
        "    fc_out_features = 12\n",
        "    seed = 0\n",
        "\n",
        "    # Running the function:\n",
        "    DeepQ = deep_q_network_learning(wenv,\n",
        "                                    max_episodes,\n",
        "                                    learning_rate,\n",
        "                                    gamma, epsilon,\n",
        "                                    batch_size,\n",
        "                                    target_update_frequency,\n",
        "                                    buffer_size,\n",
        "                                    kernel_size,\n",
        "                                    conv_out_channels,\n",
        "                                    fc_out_features,\n",
        "                                    seed)\n",
        "\n",
        "    # Obtaining the policy & state values:\n",
        "    DeepQ = wenv.decode_policy(DeepQ)\n",
        "    labels = (\"deep q network learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults([DeepQ], labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhM4W_BNx4a",
        "outputId": "3e6ee525-660e-4041-a387-9651dd05502c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER D\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '^']\n",
            " ['_' '_' '_' '_']\n",
            " ['>' '>' '_' '_']\n",
            " ['_' '>' '>' '_']]\n",
            "Value:\n",
            "[[0.526 0.480 0.592 0.312]\n",
            " [0.615 0.026 0.671 0.022]\n",
            " [0.632 0.738 0.839 0.024]\n",
            " [0.026 0.847 0.909 0.979]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE ON SETTING LEARNING RATE**:<br>If the learning rate is set too high, the gradient descent process will tend to overshoot the optimum. The result of this in our case is that the weights become highly negative overall, producing results such that applying ReLU leads to a zero-matrix. This zero-matrix leads to every resultant row of the final output (i.e. the action-values for each state) being equal, leading to a situation where:\n",
        "\n",
        "- The forward model produces the same action-values for each action, no matter the state\n",
        "- As a result of the above, the same maximum action-value is indicated for each state\n",
        "- As a result of the above, action-values & thus policy converge to the same value & same action for each state\n",
        "\n",
        "Hence, set the learning rate sufficiently low to prevent such an overshooting gradient descent."
      ],
      "metadata": {
        "id": "SDG3xkt0uK_y"
      }
    }
  ]
}
