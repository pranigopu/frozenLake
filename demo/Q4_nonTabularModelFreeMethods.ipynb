{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 4:<br>**Non-tabular Model-free Methods**"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "# Saving the present working directory's path:\n",
        "# NOTE: Change `pwd` based on your own Google Drive organisation\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "185ccc0a-dc9d-4281-d130-75a8eddb1850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "dda8b565-e1bd-4370-bc95-5a3850e88fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "JBqlpWQmq10g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "KyFzTD6Bq369"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the environment to enable feature mapping\n",
        "The class `LinearWrapper` implements a wrapper that forms a layer of abstraction around an environment object that is given to its constructor, and the wrapper class has similar functionalities as this environment object's class. However, the methods reset and step return a feature matrix when they would typically return\n",
        "a state `s`.\n",
        "<br><br>**Structure of the feature matrix**...\n",
        "<br>In general, a feature map $\\phi$ is a function that maps each state $s \\in S$ to an $m$ dimensional feature vector, i.e. $\\phi: S \\rightarrow \\mathbb{R}^m$. Hence, in general, a feature matrix can be thought of as a collection of feature vectors wherein each feature vector maps to a particular state. In other words, a feature matrix is a matrix wherein each row corresponds to a state, and each column corresponds to a particular feature; each features vector helps describe a state, and the feature matrix helps describe the collection of all states.\n",
        "<br><br>However, in our case, since the agent needs to learn the action-value function (which is based on state-action pairs), the feature map $\\phi$ is a function that maps each state-action pair $(s, a) \\in S \\times A$ to an $m$ dimensional feature vector, i.e. $\\phi: S \\times A \\rightarrow \\mathbb{R}^m$. Hence, here, the feature matrix can be thought of as a collection of feature vectors wherein each feature vector maps to a particular state-action pair. How exactly such feature mapping is implemented is discussed after the following code block."
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWrapper:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.n_actions = self.env.n_actions\n",
        "        self.n_states = self.env.n_states\n",
        "        self.n_features = self.n_states * self.n_actions\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Mapping the given state paired with each action to vectors of features:\n",
        "\n",
        "    def encode_state(self, s):\n",
        "        # Initialising the feature matrix:\n",
        "        features = np.zeros((self.n_actions, self.n_features))\n",
        "        for a in range(self.n_actions):\n",
        "            i = np.ravel_multi_index((s, a), (self.n_states, self.n_actions))\n",
        "            # Updating the feature matrix:\n",
        "            features[a, i] = 1.0\n",
        "        '''\n",
        "        EXPECTED RESULT:\n",
        "        `features` is such that each row i corresponds to an action i, each\n",
        "        column corresponds to a state-action pair (see implementation notes\n",
        "        for more clarity on the structure), and for each row i, 1.0 is assigned\n",
        "        only to those indices that correspond the given state s and action i.\n",
        "        Hence, each row vector is 1.0 at only one position and 0 in all others.\n",
        "        '''\n",
        "\n",
        "        return features\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the policy via decoding the feature matrix:\n",
        "\n",
        "    def decode_policy(self, theta):\n",
        "        # Initialising the policy & state value arrays:\n",
        "        policy = np.zeros(self.env.n_states, dtype=int)\n",
        "        value = np.zeros(self.env.n_states)\n",
        "        #------------------------------------\n",
        "        # Decoding the action-value function & obtaining policy & state values:\n",
        "        for s in range(self.n_states):\n",
        "            features = self.encode_state(s)\n",
        "            q = features.dot(theta)\n",
        "            policy[s] = np.argmax(q)\n",
        "            value[s] = np.max(q)\n",
        "            '''\n",
        "            NOTE ON THE NATURE OF `q`:\n",
        "            `q` is calculated each time w.r.t. the given state `s`; do not\n",
        "            consider it as an array mapping every possible state-action pair to\n",
        "            a value, but rather an array that, given a state, maps every action\n",
        "            to a value (effectively mapping the state-action values for the\n",
        "            particular state)\n",
        "            '''\n",
        "        #------------------------------------\n",
        "        # Returning obtained policy & state values:\n",
        "        return policy, value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Resetting environment & encoding it as feature vector:\n",
        "\n",
        "    def reset(self):\n",
        "        return self.encode_state(self.env.reset())\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Taking a step in environment & encoding next state as feature vector:\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done = self.env.step(action)\n",
        "        return self.encode_state(state), reward, done\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Visualising the agent's performance (by inputs or using a policy):\n",
        "\n",
        "    def render(self, policy=None, value=None):\n",
        "        self.env.render(policy, value)"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROGRAMMING NOTE**: `numpy.ravel_multi_index` **as used in the code**:<br>Consider the following:\n",
        "\n",
        "- $A$ is an $m \\times n$ array\n",
        "- $R$ is a vector containing certain row indices of $A$\n",
        "- $C$ is a vector containing certain column indices of $A$\n",
        "- $(R, C)$ together refer to particular cells of $A$\n",
        "\n",
        "In other terms, $R$ is vector containing indices referring to particular rows of $A$, while $C$ is a vector containing indices referring to particular columns of $A$. When put together as a tuple $(R, C)$, we have that $(R[i], C[i])$ refers to a particular element of $A$ for each $i$. Hence, $R$ and $C$ have to be of equal size; if either one is an integer, we consider it the same way we would consider a vector of equal size as the other with each element being this integer.<br>**EXAMPLE**: $([1, 2, 3], 0) \\equiv ([1, 2, 3], [0, 0, 0])$\n",
        "<br>**KEY NOTE**: If both $R$ and $C$ are integers, then $(R, C)$ refers to a single element.\n",
        "<br><br>`numpy.ravel_multi_index` accepts the following main arguments:\n",
        "\n",
        "- `multi_index` = $(V_1, V_2 ... V_k)$\n",
        "- `dims` = $(m_1, m_2 ... m_k)$\n",
        "\n",
        "In general, given a $k$-dimensional array $A$ with dimensionality $m_1 \\times m_2 \\times ... \\times m_k$, `multi_index` refers to the multi-dimensional indices wherein the $i$-th element $V_i$ of the tuple is the index or vector of indices for the $i$-th dimension of $A$. The function `numpy.ravel_multi_index` returns the indices for the same element(s) of $A$ considering a flattened (1-dimensional) version of $A$.\n",
        "<br><br>When referring to a 2-dimensional $m \\times n$ array:\n",
        "\n",
        "- `multi_index` = $(R, C)$\n",
        "- `dims` = $(m, n)$\n",
        "\n",
        "In our case in particular, the arguments become:\n",
        "\n",
        "- `multi_index` = `(s, a)` (`s` and `a` are integers)\n",
        "- `dims` = `(self.n_states, self.n_actions)`\n",
        "\n",
        "Here, `s` is the integer referring a particular state and `a` is the integer referring to a particular action. Hence, the index is $(s, a)$, and we are considering a hypothetical `n_states` $\\times$ `n_actions` array $A$ to which this index refers to. What we obtain is the corresponding index for the flattened version of this hypothetical array $A$.\n",
        "<br><br>**SIDE NOTE**: **Optional argument** `order`:<br>This argument determines whether the multi-index should be viewed as indexing in row-major (C-style) or column-major (Fortran-style) order. The former is the default and can be specified as `'C'`, while the latter is specified `'F'`. To make it clearer, row-major assumes that the array that the indices refer to is flattened in a left-to-right order starting from the first (top-most) row, whereas column-major assumes that this array is flattened in a top-to-bottom order starting from the first (left-most) column.\n",
        "\n",
        "---\n",
        "\n",
        "**IMPLEMENTATION NOTE**: **Feature mapping method**:<br>The feature matrix $M_{\\phi}$ is an $|A| \\times (|S| \\cdot |A|)$ array, where $A$ is the collection of all actions (with cardinality $|A|$) and $S$ is the collection of all states (with cardinality $|S|$). We consider each row vector as a flattened $|S| \\times |A|$ array (_flattened in row-major order by default_) that pairs each state $i$ (corresponding to row $i$) to each action $j$ (corresponding to column $j$). For convenience, let us call this row vector $V_{(s,a)}$, which is duplicated for each action in the feature matrix $M_{\\phi}$.\n",
        "<br><br>Now, considering the feature matrix $M_{\\phi}$, we initialise this matrix as follows. For each row $i$ of $M_{\\phi}$ (which corresponds to action $i$), we assign $1$ to all those indices of the $i$-th duplicate of $V_{(s,a)}$ that refer to any state paired with action $i$; we assign $0$ for all other indices. Hence, though each action is technically mapped to $|S| \\cdot |A|$ features, in practice, each state-action pair is mapped to one feature. In fact, practically, the whole feature matrix $M_{\\phi}$ is the feature map with respect to one state and all action. Furthermore, $Q$, i.e. the action reward function (represented as an array), is updated such that $Q(a)$ returns the action reward for $a$ given a predefined state $s$.\n",
        "\n",
        "<br>**Reasons for this representation**...\n",
        "<br>The above explained representation has the following advantages:\n",
        "\n",
        "- Action value array needs to be only as large as `n_actions`, since it is obtained anew given a state\n",
        "- Getting dot product between parameter vector $\\theta$ and each feature vector is made easy, since:\n",
        "  - Each feature vector is of the same size (`n_states` $\\times$ `n_actions`) as $\\theta$\n",
        "  - Each feature vector's elements correspond to those of $\\theta$\n",
        "  - Irrelevant positions in feature vector for given action-state pair are $0$, hence do not contribute\n",
        "\n",
        "**NOTE**: The dot product between the parameter vector $\\theta$ and one or all feature vector is used to obtain the action reward values $Q$ (for given a state), and is also used to update $\\theta$ for the next iteration."
      ],
      "metadata": {
        "id": "xAQ7tSGx7S1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA with linear action value function approximation"
      ],
      "metadata": {
        "id": "3tGVyyURHWee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_sarsa(wenv, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `wenv`:\n",
        "        - Wrapped object of a chosen environment model (ex. FrozenLake)\n",
        "        - Contains mechanisms to map each state to a feature vector\n",
        "        - Helpful in estimating action-value function as a linear function\n",
        "        - Also helps decode feature vectors to estimated optimal policies\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factor is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    theta = np.zeros(wenv.n_features)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda q, e: {True: random_state.randint(0,wenv.n_actions),\n",
        "                             False: np.argmax(q)}[random_state.rand() < e]\n",
        "    # NOTE 1: `q` is the array of rewards per action for a given state\n",
        "    # NOTE 2: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        features = wenv.reset()\n",
        "        # NOTE: `features` here represents the initial state\n",
        "        q = features.dot(theta)\n",
        "        # NOTE: `q` here is the rewards per action for the initial state\n",
        "        a = e_greedy(q, epsilon[i])\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            next_features, r, done = wenv.step(a)\n",
        "            # NOTE: `next_features` here represents the next state reached\n",
        "\n",
        "            # Obtaining part of the temporal difference of `(features, a)`:\n",
        "            delta = r - q[a]\n",
        "\n",
        "            # Selecting action `a` for `next_features`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            q = next_features.dot(theta)\n",
        "            # NOTE: `q` here is the rewards per action for the next state\n",
        "            next_a = e_greedy(q, epsilon[i])\n",
        "            # NOTE: `next_a` is the action taken in the next state\n",
        "\n",
        "            # Obtaining the full temporal difference of `(features, a)`:\n",
        "            delta += gamma*q[next_a]\n",
        "\n",
        "            # Updating model parameters `theta`:\n",
        "            theta += eta[i]*delta*features[a]\n",
        "            # `next_features[a]` is feature vector for state `s` & action `a`\n",
        "\n",
        "            # Moving to the next state & its corresponding action:\n",
        "            features, a = next_features, next_a\n",
        "\n",
        "    # Returning the parameter vector `theta`:\n",
        "    return theta"
      ],
      "metadata": {
        "id": "zkWyTmO9uHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning with linear action value function approximation"
      ],
      "metadata": {
        "id": "aVyZxueT1WZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_q_learning(wenv, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    Same as for the function `linear_sarsa`.\n",
        "    '''\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    theta = np.zeros(wenv.n_features)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda q, e: {True: random_state.randint(0,wenv.n_actions),\n",
        "                             False: np.argmax(q)}[random_state.rand() < e]\n",
        "    # NOTE 1: `q` is the array of rewards per action for a given state\n",
        "    # NOTE 2: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        features = wenv.reset()\n",
        "        # NOTE: `features` here represents the initial state\n",
        "        q = features.dot(theta)\n",
        "        # NOTE: `q` here is the rewards per action for the initial state\n",
        "        a = e_greedy(q, epsilon[i])\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            next_features, r, done = wenv.step(a)\n",
        "            # NOTE: `next_features` here represents the next state reached\n",
        "\n",
        "            # Obtaining part of the temporal difference of `(features, a)`:\n",
        "            delta = r - q[a]\n",
        "\n",
        "            # Selecting action `a` for `next_features`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            q = next_features.dot(theta)\n",
        "            # NOTE: `q` here is the rewards per action for the next state\n",
        "            max_a = np.argmax(q)\n",
        "            # NOTE: `max_a` is the action maximising `q` for next state\n",
        "\n",
        "            # Obtaining the full temporal difference of `(features, a)`:\n",
        "            delta += gamma*q[max_a]\n",
        "\n",
        "            # Updating model parameters `theta`:\n",
        "            theta += eta[i]*delta*features[a]\n",
        "            # `next_features[a]` is feature vector for state `s` & action `a`\n",
        "\n",
        "            # Moving to the next state & its corresponding action:\n",
        "            features, a = next_features, e_greedy(q, epsilon[i])\n",
        "\n",
        "    # Returning the parameter vector `theta`:\n",
        "    return theta"
      ],
      "metadata": {
        "id": "ikqPbOnZuMSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "hTiJ0vYqNwqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake['small'], 0.1, 100)\n",
        "    wenv = LinearWrapper(env)\n",
        "    max_episodes = 2000\n",
        "    eta = 1\n",
        "    gamma = 0.9\n",
        "    epsilon = 1\n",
        "\n",
        "    # Running the functions:\n",
        "    theta_SARSA = linear_sarsa(wenv, max_episodes, eta, gamma, epsilon)\n",
        "    theta_QLearning = linear_q_learning(wenv, max_episodes, eta, gamma, epsilon)\n",
        "\n",
        "    LSARSA = wenv.decode_policy(theta_SARSA)\n",
        "    LQLearning = wenv.decode_policy(theta_QLearning)\n",
        "    labels = (\"linear sarsa\", \"linear q-learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults((LSARSA, LQLearning), labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhM4W_BNx4a",
        "outputId": "368a2a0a-ebb4-46e7-8017-79bfb12533d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER LINEAR SARSA\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '<']]\n",
            "Value:\n",
            "[[0.426 0.344 0.472 0.149]\n",
            " [0.485 0.000 0.580 0.000]\n",
            " [0.558 0.668 0.775 0.000]\n",
            " [0.000 0.785 0.882 1.000]]\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER LINEAR Q-LEARNING\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['>' '>' '_' '<']\n",
            " ['^' '^' '_' '^']\n",
            " ['^' '>' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.468 0.524 0.585 0.532]\n",
            " [0.376 0.000 0.673 0.000]\n",
            " [0.331 0.660 0.788 0.000]\n",
            " [0.000 0.769 0.883 1.000]]\n"
          ]
        }
      ]
    }
  ]
}
