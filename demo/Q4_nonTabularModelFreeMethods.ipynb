{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 4:<br>**Non-tabular Model-free Methods**"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Mounting the Google Drive folder (run if necessary):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    # Saving the present working directory's path:\n",
        "    # NOTE: Change `pwd` based on your own Google Drive organisation\n",
        "    pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "09086753-3d73-45c5-9ca0-5c11673800bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import import_ipynb\n",
        "    N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "    N.load_module(\"Q1_environment\")\n",
        "    from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "bcc3b413-ed5e-41d9-a525-7dbc8d1aadf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: `Q1_environment` contains\n",
        "\n",
        "- The environment class and `FrozenLake` subclass\n",
        "- The lists containing the small and big lake environments\n",
        "- The function for rendering the policy and state-values\n",
        "- The function for implementing the epsilon-greedy policy"
      ],
      "metadata": {
        "id": "24jCXJ9R4iZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "JBqlpWQmq10g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "KyFzTD6Bq369"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\epsilon$-greedy policy\n",
        "The $\\epsilon$-greedy (epsilon-greedy) policy is a method of choosing an action from a state such that the probability of choosing a random action is $\\epsilon$ while the probability of choosing the action that maximises the action-value function (as far as it has been estimated) from the given state is $1-\\epsilon$. $\\epsilon$ (epsilon) is called the \"exploration factor\". $\\epsilon$-greedy is a solution to the exploration-exploitation dilemma, wherein the degree of exploration (i.e. expansion of actions unknown potential) is decided by the exploration factor.\n",
        "\n",
        "<br>**NOTE**: **Breaking ties between reward-maximising actions**:\n",
        "<br>When the above policy chooses to exploit rather than explore, it may be the case that there exist multiple actions that (within a tolerance level) maximise the action-value function from a given state. In such a case, exploration can be encouraged even during exploitation by making a random selection from the reward-maximising actions. This approach is _no worse and potentially better_ than simply picking the first action that maximises the action-value function from the given state, since it furthers exploration without reducing exploitation."
      ],
      "metadata": {
        "id": "Hlb6J8TuxN5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: The following is generalised for the tabular & non-tabular methods:\n",
        "def e_greedy(q, e, n_actions, random_state, s=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `q`: One of the following...\n",
        "        - The matrix of action-values for each state\n",
        "        - The array of action-values for a given state\n",
        "    - `e`: The exploration factor epsilon\n",
        "    - `n_actions`: The number of actions an agent can take from any state\n",
        "    - `random_state`: The set `numpy.random.RandomState` object\n",
        "    - `s`: The given state\n",
        "\n",
        "    If `s=None`, `q` is the array of action-values for a given state. Else,\n",
        "    `q` is the matrix of action-values for each state.\n",
        "    '''\n",
        "\n",
        "    # Storing the action-values for the given state `s` (if `s` is given):\n",
        "    if s != None: q = q[s]\n",
        "\n",
        "    # The epsilon-greedy policy:\n",
        "    if random_state.rand() < e:\n",
        "        return random_state.randint(0,n_actions)\n",
        "    else:\n",
        "        # Obtaining all actions that maximise action-value from state `s`:\n",
        "        best = [a for a in range(n_actions) if np.allclose(np.max(q), q[a])]\n",
        "        # Breaking the tie using random selection:\n",
        "        return random_state.choice(best)"
      ],
      "metadata": {
        "id": "MA4NedCHxT04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the environment to enable feature mapping\n",
        "The class `LinearWrapper` implements a wrapper that forms a layer of abstraction around an environment object that is given to its constructor, and the wrapper class has similar functionalities as this environment object's class. However, the methods reset and step return a feature matrix when they would typically return\n",
        "a state `s`.\n",
        "<br><br>**Structure of the feature matrix**...\n",
        "<br>In general, a feature map $\\phi$ is a function that maps each state $s \\in S$ to an $m$ dimensional feature vector, i.e. $\\phi: S \\rightarrow \\mathbb{R}^m$. Hence, in general, a feature matrix can be thought of as a collection of feature vectors wherein each feature vector maps to a particular state. In other words, a feature matrix is a matrix wherein each row corresponds to a state, and each column corresponds to a particular feature; each features vector helps describe a state, and the feature matrix helps describe the collection of all states.\n",
        "<br><br>However, in our case, since the agent needs to learn the action-value function (which is based on state-action pairs), the feature map $\\phi$ is a function that maps each state-action pair $(s, a) \\in S \\times A$ to an $m$ dimensional feature vector, i.e. $\\phi: S \\times A \\rightarrow \\mathbb{R}^m$. Hence, here, the feature matrix can be thought of as a collection of feature vectors wherein each feature vector maps to a particular state-action pair. How exactly such feature mapping is implemented is discussed after the following code block."
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWrapper:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.n_actions = self.env.n_actions\n",
        "        self.n_states = self.env.n_states\n",
        "        self.n_features = self.n_states * self.n_actions\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Mapping the given state paired with each action to vectors of features:\n",
        "\n",
        "    def encode_state(self, s):\n",
        "        # Initialising the feature matrix:\n",
        "        features = np.zeros((self.n_actions, self.n_features))\n",
        "        for a in range(self.n_actions):\n",
        "            i = np.ravel_multi_index((s, a), (self.n_states, self.n_actions))\n",
        "            # Updating the feature matrix:\n",
        "            features[a, i] = 1.0\n",
        "        '''\n",
        "        EXPECTED RESULT:\n",
        "        `features` is such that each row i corresponds to an action i, each\n",
        "        column corresponds to a state-action pair (see implementation notes\n",
        "        for more clarity on the structure), and for each row i, 1.0 is assigned\n",
        "        only to those indices that correspond the given state s and action i.\n",
        "        Hence, each row vector is 1.0 at only one position and 0 in all others.\n",
        "        '''\n",
        "\n",
        "        return features\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the policy via decoding the feature matrix:\n",
        "\n",
        "    def decode_policy(self, theta):\n",
        "        # Initialising the policy & state value arrays:\n",
        "        policy = np.zeros(self.env.n_states, dtype=int)\n",
        "        value = np.zeros(self.env.n_states)\n",
        "        #------------------------------------\n",
        "        # Decoding the action-value function & obtaining policy & state values:\n",
        "        for s in range(self.n_states):\n",
        "            features = self.encode_state(s)\n",
        "            q = features.dot(theta)\n",
        "            policy[s] = np.argmax(q)\n",
        "            value[s] = np.max(q)\n",
        "            '''\n",
        "            NOTE ON THE NATURE OF `q`:\n",
        "            `q` is calculated each time w.r.t. the given state `s`; do not\n",
        "            consider it as an array mapping every possible state-action pair to\n",
        "            a value, but rather an array that, given a state, maps every action\n",
        "            to a value (effectively mapping the state-action values for the\n",
        "            particular state)\n",
        "            '''\n",
        "        #------------------------------------\n",
        "        # Returning obtained policy & state values:\n",
        "        return policy, value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Resetting environment & encoding it as feature vector:\n",
        "\n",
        "    def reset(self):\n",
        "        return self.encode_state(self.env.reset())\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Taking a step in environment & encoding next state as feature vector:\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done = self.env.step(action)\n",
        "        return self.encode_state(state), reward, done\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Visualising the agent's performance (by inputs or using a policy):\n",
        "\n",
        "    def render(self, policy=None, value=None):\n",
        "        self.env.render(policy, value)"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROGRAMMING NOTE**: `numpy.ravel_multi_index` **as used in the code**:<br>Consider the following:\n",
        "\n",
        "- $A$ is an $m \\times n$ array\n",
        "- $R$ is a vector containing certain row indices of $A$\n",
        "- $C$ is a vector containing certain column indices of $A$\n",
        "- $(R, C)$ together refer to particular cells of $A$\n",
        "\n",
        "In other terms, $R$ is vector containing indices referring to particular rows of $A$, while $C$ is a vector containing indices referring to particular columns of $A$. When put together as a tuple $(R, C)$, we have that $(R[i], C[i])$ refers to a particular element of $A$ for each $i$. Hence, $R$ and $C$ have to be of equal size; if either one is an integer, we consider it the same way we would consider a vector of equal size as the other with each element being this integer.<br>**EXAMPLE**: $([1, 2, 3], 0) \\equiv ([1, 2, 3], [0, 0, 0])$\n",
        "<br>**KEY NOTE**: If both $R$ and $C$ are integers, then $(R, C)$ refers to a single element.\n",
        "<br><br>`numpy.ravel_multi_index` accepts the following main arguments:\n",
        "\n",
        "- `multi_index` = $(V_1, V_2 ... V_k)$\n",
        "- `dims` = $(m_1, m_2 ... m_k)$\n",
        "\n",
        "In general, given a $k$-dimensional array $A$ with dimensionality $m_1 \\times m_2 \\times ... \\times m_k$, `multi_index` refers to the multi-dimensional indices wherein the $i$-th element $V_i$ of the tuple is the index or vector of indices for the $i$-th dimension of $A$. The function `numpy.ravel_multi_index` returns the indices for the same element(s) of $A$ considering a flattened (1-dimensional) version of $A$.\n",
        "<br><br>When referring to a 2-dimensional $m \\times n$ array:\n",
        "\n",
        "- `multi_index` = $(R, C)$\n",
        "- `dims` = $(m, n)$\n",
        "\n",
        "In our case in particular, the arguments become:\n",
        "\n",
        "- `multi_index` = `(s, a)` (`s` and `a` are integers)\n",
        "- `dims` = `(self.n_states, self.n_actions)`\n",
        "\n",
        "Here, `s` is the integer referring a particular state and `a` is the integer referring to a particular action. Hence, the index is $(s, a)$, and we are considering a hypothetical `n_states` $\\times$ `n_actions` array $A$ to which this index refers to. What we obtain is the corresponding index for the flattened version of this hypothetical array $A$.\n",
        "<br><br>**SIDE NOTE**: **Optional argument** `order`:<br>This argument determines whether the multi-index should be viewed as indexing in row-major (C-style) or column-major (Fortran-style) order. The former is the default and can be specified as `'C'`, while the latter is specified `'F'`. To make it clearer, row-major assumes that the array that the indices refer to is flattened in a left-to-right order starting from the first (top-most) row, whereas column-major assumes that this array is flattened in a top-to-bottom order starting from the first (left-most) column.\n",
        "\n",
        "---\n",
        "\n",
        "**IMPLEMENTATION NOTE**: **Feature mapping method**:<br>The feature matrix $M_{\\phi}$ is an $|A| \\times (|S| \\cdot |A|)$ array, where $A$ is the collection of all actions (with cardinality $|A|$) and $S$ is the collection of all states (with cardinality $|S|$). We consider each row vector as a flattened $|S| \\times |A|$ array (_flattened in row-major order by default_) that pairs each state $i$ (corresponding to row $i$) to each action $j$ (corresponding to column $j$). For convenience, let us call this row vector $V_{(s,a)}$, which is duplicated for each action in the feature matrix $M_{\\phi}$.\n",
        "<br><br>Now, considering the feature matrix $M_{\\phi}$, we initialise this matrix as follows. For each row $i$ of $M_{\\phi}$ (which corresponds to action $i$), we assign $1$ to all those indices of the $i$-th duplicate of $V_{(s,a)}$ that refer to any state paired with action $i$; we assign $0$ for all other indices. Hence, though each action is technically mapped to $|S| \\cdot |A|$ features, in practice, each state-action pair is mapped to one feature. In fact, practically, the whole feature matrix $M_{\\phi}$ is the feature map with respect to one state and all actions. Furthermore, $Q$, i.e. the action reward function (represented as an array), is updated such that $Q(a)$ returns the action reward for $a$ given a predefined state $s$.\n",
        "\n",
        "<br>**Reasons for this representation**...\n",
        "<br>The above explained representation has the following advantages:\n",
        "\n",
        "- Action value array needs to be only as large as `n_actions`, since it is obtained anew given a state\n",
        "- Getting dot product between parameter vector $\\theta$ and each feature vector is made easy, since:\n",
        "  - Each feature vector is of the same size (`n_states` $\\times$ `n_actions`) as $\\theta$\n",
        "  - Each feature vector's elements correspond to those of $\\theta$\n",
        "  - Irrelevant positions in feature vector for given action-state pair are $0$, hence do not contribute\n",
        "\n",
        "**NOTE**: The dot products between the parameter vector $\\theta$ and the feature vectors are used to obtain the action reward values $Q$ (for given a state), and are also used to update $\\theta$ for the next iteration."
      ],
      "metadata": {
        "id": "xAQ7tSGx7S1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA with linear action value function approximation"
      ],
      "metadata": {
        "id": "3tGVyyURHWee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_sarsa(wenv, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `wenv`:\n",
        "        - Wrapped object of a chosen environment model (ex. FrozenLake)\n",
        "        - Contains mechanisms to map each state to a feature vector\n",
        "        - Helpful in estimating action-value function as a linear function\n",
        "        - Also helps decode feature vectors to estimated optimal policies\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factor is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    theta = np.zeros(wenv.n_features)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        features = wenv.reset()\n",
        "        # NOTE: `features` here represents the initial state\n",
        "        q = features.dot(theta)\n",
        "        # NOTE: `q` here is the rewards per action for the initial state\n",
        "        '''\n",
        "        NOTE ON THE SHAPE OF `q`:\n",
        "        `features.dot(theta)` produces `wenv.n_actions` dot products by\n",
        "        applying dot product between `theta` and each row vector of the matrix\n",
        "        `features`.\n",
        "        '''\n",
        "        a = e_greedy(q, epsilon[i], wenv.n_actions, random_state)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            next_features, r, done = wenv.step(a)\n",
        "            # NOTE: `next_features` here represents the next state reached\n",
        "\n",
        "            # Obtaining part of the temporal difference of `(features, a)`:\n",
        "            delta = r - q[a]\n",
        "\n",
        "            # Selecting action `a` for `next_features`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            q = next_features.dot(theta)\n",
        "            # NOTE: `q` here is the rewards per action for the next state\n",
        "            next_a = e_greedy(q, epsilon[i], wenv.n_actions, random_state)\n",
        "            # NOTE: `next_a` is the action taken in the next state\n",
        "\n",
        "            # Obtaining the full temporal difference of `(features, a)`:\n",
        "            delta += gamma*q[next_a]\n",
        "\n",
        "            # Updating model parameters `theta`:\n",
        "            theta += eta[i]*delta*features[a]\n",
        "            # `next_features[a]` is feature vector for state `s` & action `a`\n",
        "\n",
        "            # Moving to the next state & its corresponding action:\n",
        "            features, a = next_features, next_a\n",
        "\n",
        "    # Returning the parameter vector `theta`:\n",
        "    return theta"
      ],
      "metadata": {
        "id": "zkWyTmO9uHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning with linear action value function approximation"
      ],
      "metadata": {
        "id": "aVyZxueT1WZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_q_learning(wenv, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    Same as for the function `linear_sarsa`.\n",
        "    '''\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    theta = np.zeros(wenv.n_features)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        features = wenv.reset()\n",
        "        # NOTE: `features` here represents the initial state\n",
        "        q = features.dot(theta)\n",
        "        # NOTE: `q` here is the rewards per action for the initial state\n",
        "        '''\n",
        "        NOTE ON THE SHAPE OF `q`:\n",
        "        See the corresponding comment for the function `linear_sarsa`.\n",
        "        '''\n",
        "        a = e_greedy(q, epsilon[i], wenv.n_actions, random_state)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            next_features, r, done = wenv.step(a)\n",
        "            # NOTE: `next_features` here represents the next state reached\n",
        "\n",
        "            # Obtaining part of the temporal difference of `(features, a)`:\n",
        "            delta = r - q[a]\n",
        "\n",
        "            # Selecting action `a` for `next_features`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            q = next_features.dot(theta)\n",
        "            # NOTE: `q` here is the rewards per action for the next state\n",
        "            max_a = np.argmax(q)\n",
        "            # NOTE: `max_a` is the action maximising `q` for next state\n",
        "\n",
        "            # Obtaining the full temporal difference of `(features, a)`:\n",
        "            delta += gamma*q[max_a]\n",
        "\n",
        "            # Updating model parameters `theta`:\n",
        "            theta += eta[i]*delta*features[a]\n",
        "            # `next_features[a]` is feature vector for state `s` & action `a`\n",
        "\n",
        "            # Moving to the next state & its corresponding action:\n",
        "            features, a = next_features, e_greedy(q, epsilon[i], wenv.n_actions, random_state)\n",
        "\n",
        "    # Returning the parameter vector `theta`:\n",
        "    return theta"
      ],
      "metadata": {
        "id": "ikqPbOnZuMSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "hTiJ0vYqNwqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake=LAKE['small'], slip=0.1, max_steps=None, seed=0)\n",
        "    # NOTE: Putting `max_steps=None` makes it default to the grid size\n",
        "    wenv = LinearWrapper(env)\n",
        "    max_episodes = 2000\n",
        "    eta = 1\n",
        "    gamma = 0.9\n",
        "    epsilon = 1\n",
        "\n",
        "    # Running the functions:\n",
        "    _LSARSA = linear_sarsa(wenv, max_episodes, eta, gamma, epsilon, 0)\n",
        "    _LQLearning = linear_q_learning(wenv, max_episodes, eta, gamma, epsilon, 0)\n",
        "\n",
        "    LSARSA = wenv.decode_policy(_LSARSA)\n",
        "    LQLearning = wenv.decode_policy(_LQLearning)\n",
        "    labels = (\"linear sarsa\", \"linear q-learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults((LSARSA, LQLearning), labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhM4W_BNx4a",
        "outputId": "2ea7ee15-26f6-4383-aecd-e559deb8a87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER LINEAR SARSA\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '_']]\n",
            "Value:\n",
            "[[0.365 0.413 0.494 0.346]\n",
            " [0.435 0.000 0.596 0.000]\n",
            " [0.542 0.638 0.761 0.000]\n",
            " [0.000 0.776 0.890 1.000]]\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER LINEAR Q-LEARNING\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '<' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '_']]\n",
            "Value:\n",
            "[[0.427 0.386 0.369 0.359]\n",
            " [0.482 0.000 0.541 0.000]\n",
            " [0.573 0.685 0.784 0.000]\n",
            " [0.000 0.739 0.891 1.000]]\n"
          ]
        }
      ]
    }
  ]
}