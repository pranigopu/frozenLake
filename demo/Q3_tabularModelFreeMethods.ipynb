{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 3:<br>**Tabular Model-free Methods**\n",
        "\n",
        "## Introduction to the concept\n",
        "Model-based methods are algorithms that cannot use (thus do not require) a known environment model. \"Tabular\" refers to the use of look-up tables or arrays that store previous estimates of a value function (ex. state value function) and use these estimates to update them every iteration. It is not surprising that when the environment model is not known to begin with, i.e. when the state-transition probabilities and rewards are initially unknown for each state-action pair, the evaluation and improvement of policies would take more time to be accurate and effective, since the agent has to learn the rewards for each state-action pair by iteracting with the environment repeatedly."
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "# Saving the present working directory's path:\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "1bac7cfb-d821-4e2f-b05d-d86312d5e26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "import numpy as np\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "cf3ef50b-2f2a-45fc-a6c2-e43fef3a7a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA control\n",
        "**NOTE**: SARSA $\\implies$ \"State-Action-Reward-State-Action\""
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factor is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Choosing a random initial state:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    # Initialising array of learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # Initialising array of exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # Initialising of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    The above `eta` and `epsilon` are arrays formed by taking the initial\n",
        "    learning rate and exploration factor given and creating an array of\n",
        "    linearly decreasing values. Hence:\n",
        "    - eta[i] is the learning rate for the ith episode\n",
        "    - epsilon[i] is the exploration factor for the ith episode\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda s, e: {True: np.random.randint(0,env.n_actions),\n",
        "                             False: np.argmax(q[s])}[np.random.rand() < e]\n",
        "    # NOTE: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(s, epsilon[i])\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        A terminal state is one wherein either the maximum number\n",
        "        of steps for taking actions is reached or the agent reaches the\n",
        "        absorbing state or the agent transitions to the absorbing state for any\n",
        "        action. In this implementation, the check for whether the terminal\n",
        "        state is reached is handled by the `done` flag of the `env.step`\n",
        "        function; if `False`, continue, else consider the state as terminal.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Selecting action `next_a` for `next_s`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            next_a = e_greedy(next_s, epsilon[i])\n",
        "\n",
        "            # Updating the action reward for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with epsilon-greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*q[next_s, next_a] - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, next_a\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "w-hrjB2nKg3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    Same as for the function `sarsa`.\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Choosing a random state:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    # Initialising array of learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # Initialising array of exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # Initialising of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    Check corresponding comment for the function `sarsa`.\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda s, e: {True: np.random.randint(0, env.n_actions),\n",
        "                             False: np.argmax(q[s])}[np.random.rand() < e]\n",
        "    # NOTE: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(s, epsilon[i])\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        Check corresponding comment for the function `sarsa`.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Updating the action reward for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*np.max(q[next_s]) - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, e_greedy(s, epsilon[i])\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "3inJplt1Ki7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "6l3q3U0ZMfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake['small'], 0.1, 100)\n",
        "    max_episodes = 2000\n",
        "    eta = 1\n",
        "    gamma = 0.9\n",
        "    epsilon = 1\n",
        "\n",
        "    # Running the functions:\n",
        "    SARSA = sarsa(env, max_episodes, eta, gamma, epsilon)\n",
        "    QLearning = q_learning(env, max_episodes, eta, gamma, epsilon)\n",
        "    labels = (\"sarsa\", \"q-learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults((SARSA, QLearning), labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42VoVYBMgub",
        "outputId": "f71f0978-9fa7-4c8d-80f0-da66fdbef731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER SARSA\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '_']]\n",
            "Value:\n",
            "[[0.430 0.383 0.456 0.346]\n",
            " [0.492 0.000 0.597 0.000]\n",
            " [0.546 0.658 0.762 0.000]\n",
            " [0.000 0.776 0.887 1.000]]\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER Q-LEARNING\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '>']]\n",
            "Value:\n",
            "[[0.496 0.467 0.602 0.545]\n",
            " [0.554 0.000 0.577 0.000]\n",
            " [0.637 0.715 0.721 0.000]\n",
            " [0.000 0.801 0.869 1.000]]\n"
          ]
        }
      ]
    }
  ]
}