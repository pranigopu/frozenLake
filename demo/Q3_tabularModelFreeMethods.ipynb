{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 3:<br>**Tabular Model-free Methods**\n",
        "\n",
        "## Introduction to the concept\n",
        "Model-based methods are algorithms that cannot use (thus do not require) a known environment model. \"Tabular\" refers to the use of look-up tables or arrays that store previous estimates of a value function (ex. state value function) and use these estimates to update them every iteration. It is not surprising that when the environment model is not known to begin with, i.e. when the state-transition probabilities and rewards are initially unknown for each state-action pair, the evaluation and improvement of policies would take more time to be accurate and effective, since the agent has to learn the rewards for each state-action pair by interacting with the environment repeatedly."
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Mounting the Google Drive folder (run if necessary):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    # Saving the present working directory's path:\n",
        "    pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "376865f7-e68a-4df2-efbe-45b06de1f43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import import_ipynb\n",
        "    N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "    N.load_module(\"Q1_environment\")\n",
        "    from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "35e6ce36-dcb9-4b4e-f2bf-aec0208266aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: `Q1_environment` contains\n",
        "\n",
        "- The environment class and `FrozenLake` subclass\n",
        "- The lists containing the small and big lake environments\n",
        "- The function for rendering the policy and state-values\n",
        "- The function for implementing the epsilon-greedy policy"
      ],
      "metadata": {
        "id": "43FZX1bN5IX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "ClGRoWHIrKVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "m14X2hvwrMfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\epsilon$-greedy policy\n",
        "The $\\epsilon$-greedy (epsilon-greedy) policy is a method of choosing an action from a state such that the probability of choosing a random action is $\\epsilon$ while the probability of choosing the action that maximises the action-value function (as far as it has been estimated) from the given state is $1-\\epsilon$. $\\epsilon$ (epsilon) is called the \"exploration factor\". $\\epsilon$-greedy is a solution to the exploration-exploitation dilemma, wherein the degree of exploration (i.e. expansion of actions unknown potential) is decided by the exploration factor.\n",
        "\n",
        "<br>**NOTE**: **Breaking ties between reward-maximising actions**:\n",
        "<br>When the above policy chooses to exploit rather than explore, it may be the case that there exist multiple actions that (within a tolerance level) maximise the action-value function from a given state. In such a case, exploration can be encouraged even during exploitation by making a random selection from the reward-maximising actions. This approach is _no worse and potentially better_ than simply picking the first action that maximises the action-value function from the given state, since it furthers exploration without reducing exploitation."
      ],
      "metadata": {
        "id": "Hlb6J8TuxN5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: The following is generalised for the tabular & non-tabular methods:\n",
        "def e_greedy(q, e, n_actions, random_state, s=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `q`: One of the following...\n",
        "        - The matrix of action-values for each state\n",
        "        - The array of action-values for a given state\n",
        "    - `e`: The exploration factor epsilon\n",
        "    - `n_actions`: The number of actions an agent can take from any state\n",
        "    - `random_state`: The set `numpy.random.RandomState` object\n",
        "    - `s`: The given state\n",
        "\n",
        "    If `s=None`, `q` is the array of action-values for a given state. Else,\n",
        "    `q` is the matrix of action-values for each state.\n",
        "    '''\n",
        "\n",
        "    # Storing the action-values for the given state `s` (if `s` is given):\n",
        "    if s != None: q = q[s]\n",
        "\n",
        "    # The epsilon-greedy policy:\n",
        "    if random_state.rand() < e:\n",
        "        return random_state.randint(0,n_actions)\n",
        "    else:\n",
        "        # Obtaining all actions that maximise action-value from state `s`:\n",
        "        best = [a for a in range(n_actions) if np.allclose(np.max(q), q[a])]\n",
        "        # Breaking the tie using random selection:\n",
        "        return random_state.choice(best)"
      ],
      "metadata": {
        "id": "MA4NedCHxT04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA control\n",
        "**NOTE**: SARSA $\\implies$ \"State-Action-Reward-State-Action\""
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factor is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    # 1. Array of linearly decreasing learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # 2. Array of linearly decreasing exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # 3. Array of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    The above `eta` and `epsilon` are arrays formed by taking the initial\n",
        "    learning rate and exploration factor given and creating an array of\n",
        "    linearly decreasing values. Hence:\n",
        "    - eta[i] is the learning rate for the ith episode\n",
        "    - epsilon[i] is the exploration factor for the ith episode\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(q, epsilon[i], env.n_actions, random_state, s)\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        A terminal state is one wherein either the maximum number\n",
        "        of steps for taking actions is reached or the agent reaches the\n",
        "        absorbing state or the agent transitions to the absorbing state for any\n",
        "        action. In this implementation, the check for whether the terminal\n",
        "        state is reached is handled by the `done` flag of the `env.step`\n",
        "        function; if `False`, continue, else consider the state as terminal.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Selecting action `next_a` for `next_s`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            next_a = e_greedy(q, epsilon[i], env.n_actions, random_state, next_s)\n",
        "\n",
        "            # Updating the action-value for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with epsilon-greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*q[next_s, next_a] - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, next_a\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "w-hrjB2nKg3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    Same as for the function `sarsa`.\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Setting random state with `seed` for enabling replicability:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    # Initialising key parameters:\n",
        "    # 1. Array of linearly decreasing learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # 2. Array of linearly decreasing exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # 3. Array of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    Check corresponding comment for the function `sarsa`.\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(q, epsilon[i], env.n_actions, random_state, s)\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        Check corresponding comment for the function `sarsa`.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Updating the action-value for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*np.max(q[next_s]) - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, e_greedy(q, epsilon[i], env.n_actions, random_state, s)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "3inJplt1Ki7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "6l3q3U0ZMfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Defining the parameters:\n",
        "    env = FrozenLake(lake=LAKE['small'], slip=0.1, max_steps=None, seed=0)\n",
        "    # NOTE: Putting `max_steps=None` makes it default to the grid size\n",
        "    max_episodes = 2000\n",
        "    eta = 1\n",
        "    gamma = 0.9\n",
        "    epsilon = 1\n",
        "\n",
        "    # Running the functions:\n",
        "    SARSA = sarsa(env, max_episodes, eta, gamma, epsilon, 0)\n",
        "    QLearning = q_learning(env, max_episodes, eta, gamma, epsilon, 0)\n",
        "    labels = (\"sarsa\", \"q-learning\")\n",
        "\n",
        "    # Displaying results:\n",
        "    displayResults((SARSA, QLearning), labels, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42VoVYBMgub",
        "outputId": "2ee1043c-9134-4a0f-8c15-2341799c3dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER SARSA\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '<' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '_']]\n",
            "Value:\n",
            "[[0.386 0.268 0.320 0.243]\n",
            " [0.463 0.000 0.489 0.000]\n",
            " [0.555 0.642 0.767 0.000]\n",
            " [0.000 0.745 0.886 1.000]]\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER Q-LEARNING\n",
            "\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '>']]\n",
            "Value:\n",
            "[[0.502 0.489 0.635 0.568]\n",
            " [0.561 0.000 0.701 0.000]\n",
            " [0.647 0.720 0.798 0.000]\n",
            " [0.000 0.798 0.892 1.000]]\n"
          ]
        }
      ]
    }
  ]
}
