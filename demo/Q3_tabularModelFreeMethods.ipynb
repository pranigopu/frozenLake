{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Question 3:<br>**Tabular Model-free Methods**"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# Saving the present working directory's path:\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "5d97ca62-fd83-450b-e1c3-60602e57f10a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "import numpy as np\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "from Q1_environment import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "eb309463-1fbb-4c22-84a9-afff0c8eaa3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA control\n",
        "**NOTE**: SARSA $\\implies$ \"State-Action-Reward-State-Action\""
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factpr is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Choosing a random initial state:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    # Initialising array of learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # Initialising array of exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # Initialising of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    The above `eta` and `epsilon` are arrays formed by taking the initial\n",
        "    learning rate and exploration factor given and creating an array of\n",
        "    linearly decreasing values. Hence:\n",
        "    - eta[i] is the learning rate for the ith episode\n",
        "    - epsilon[i] is the exploration factor for the ith episode\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda s, e: {True: np.random.randint(0, env.n_actions),\n",
        "                             False: int(np.max(q[s]))}[np.random.rand() < e]\n",
        "    # NOTE: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(s, epsilon[i])\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        A terminal state is one wherein either the maximum number\n",
        "        of steps for taking actions is reached or the agent reaches the\n",
        "        absorbing state or the agent transitions to the absorbing state for any\n",
        "        action. In this implementation, the check for whether the terminal\n",
        "        state is reached is handled by the `done` flag of the `env.step`\n",
        "        function; if `False`, continue, else consider the state as terminal.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Selecting action `next_a` for `next_s`:\n",
        "            # NOTE: Selection is done by epsilon greedy policy based on `q`\n",
        "            next_a = e_greedy(next_s, epsilon[i])\n",
        "\n",
        "            # Updating the action reward for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with epsilon-greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*q[next_s, next_a] - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, next_a\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "ntwseZZJeXzE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "w-hrjB2nKg3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    - `env`: Object of the chosen environment class (ex. FrozenLake)\n",
        "    - `max_episodes`: Upper limit of episodes the agent can go through\n",
        "    - `eta`:\n",
        "        - Initial learning rate\n",
        "        - The learning rate is meant to decrease linearly over episodes\n",
        "    - `gamma`:\n",
        "        - Discount factor\n",
        "        - Not subject to change over time\n",
        "    - `epsilon`:\n",
        "        - Initial exploration factor\n",
        "        - Exploration factor is w.r.t. epsilon-greedy policy\n",
        "        - Denotes the chance of selecting a random state\n",
        "        - The exploration factpr is meant to decrease linearly over episodes\n",
        "    - `seed`:\n",
        "        - Optional seed for pseudorandom number generation\n",
        "        - By default, it is `None` ==> random seed will be chosen\n",
        "    '''\n",
        "    # INITIALISATION\n",
        "    # Choosing a random state:\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    # Initialising array of learning rates:\n",
        "    eta = np.linspace(eta, 0, max_episodes)\n",
        "    # Initialising array of exploration factors:\n",
        "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
        "    # Initialising of state-action values:\n",
        "    q = np.zeros((env.n_states, env.n_actions))\n",
        "    '''\n",
        "    NOTE ON THE NEW `eta` & `epsilon`:\n",
        "    The above `eta` and `epsilon` are arrays formed by taking the initial\n",
        "    learning rate and exploration factor given and creating an array of\n",
        "    linearly decreasing values. Hence:\n",
        "    - eta[i] is the learning rate for the ith episode\n",
        "    - epsilon[i] is the exploration factor for the ith episode\n",
        "    '''\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # EPSILON-GREEDY POLICY\n",
        "    # Implementing the epsilon-greedy policy as a lambda function:\n",
        "    e_greedy = lambda s, e: {True: np.random.randint(0, env.n_actions),\n",
        "                             False: int(np.max(q[s]))}[np.random.rand() < e]\n",
        "    # NOTE: `e` is the given epsilon value\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # LEARNING LOOP\n",
        "    for i in range(max_episodes):\n",
        "        # NOTE: i ==> episode number\n",
        "        # Beginning at the initial state before each episode:\n",
        "        s = env.reset()\n",
        "        # Selecting action `a` for `s` by epsilon-greedy policy based on `q`:\n",
        "        a = e_greedy(s, epsilon[i])\n",
        "        # While the state is not terminal:\n",
        "        '''\n",
        "        HOW TO CHECK IF A STATE IS TERMINAL?\n",
        "        A terminal state is one wherein either the maximum number\n",
        "        of steps for taking actions is reached or the agent reaches the\n",
        "        absorbing state or the agent transitions to the absorbing state for any\n",
        "        action. In this implementation, the check for whether the terminal\n",
        "        state is reached is handled by the `done` flag of the `env.step`\n",
        "        function; if `False`, continue, else consider the state as terminal.\n",
        "        '''\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Next state & reward after taking `a` from `s`:\n",
        "            next_s, r, done = env.step(a)\n",
        "            # NOTE: `env.step` automatically updates the state of the agent\n",
        "\n",
        "            # Updating the action reward for the current state-action pair:\n",
        "            # USING: Temporal difference for (s,a) with greedy policy\n",
        "            q[s, a] = q[s, a] + eta[i]*(r + gamma*np.max(q[next_s]) - q[s, a])\n",
        "\n",
        "            # Moving to the next state & action pair:\n",
        "            s, a = next_s, e_greedy(s, epsilon[i])\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    # Obtaining the estimated optimal policy\n",
        "    # NOTE: Policy = The column index (i.e. action) with max value per row\n",
        "    policy = q.argmax(axis=1)\n",
        "    # Obtaining the state values w.r.t the above policy:\n",
        "    # NOTE: Value = Max value per row\n",
        "    value = q.max(axis=1)\n",
        "    # Returning the above obtained policy & state value array:\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "3inJplt1Ki7T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the above functions\n",
        "_The function testing code must not run if this file is imported as a module, hence we do..._<br>`if __name__ == '__main__'`<br>_... to check if the current file is being executed as the main code._"
      ],
      "metadata": {
        "id": "6l3q3U0ZMfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # Defining the parameters:\n",
        "  env = FrozenLake(lake['small'], 0.1, 100)\n",
        "  max_episodes = 2000\n",
        "  eta = 0.3\n",
        "  gamma = 0.9\n",
        "  epsilon = 0.8\n",
        "\n",
        "  # Running the functions:\n",
        "  SARSA = sarsa(env, max_episodes, eta, gamma, epsilon)\n",
        "  QLearning = q_learning(env, max_episodes, eta, gamma, epsilon)\n",
        "\n",
        "  print('RESULTS FROM SARSA')\n",
        "  print(f'Policy:\\n{SARSA[0]}\\nState values:\\n{SARSA[1]}')\n",
        "  print('------------------------------------')\n",
        "  print('RESULTS FROM Q-LEARNING')\n",
        "  print(f'Policy:\\n{QLearning[0]}\\nState values:\\n{QLearning[1]}')\n",
        "\n",
        "  print('\\n================================================\\n')\n",
        "\n",
        "  print('AGENT PERFORMANCE AFTER SARSA\\n')\n",
        "  state = env.reset()\n",
        "  for a in SARSA[0]:\n",
        "      state, r, done = env.step(a)\n",
        "      env.render()\n",
        "      print('Reward: {0}.'.format(r))\n",
        "      if done: break\n",
        "\n",
        "  print('\\n================================================\\n')\n",
        "\n",
        "  print('AGENT PERFORMANCE AFTER Q-LEARNING\\n')\n",
        "  state = env.reset()\n",
        "  for a in QLearning[0]:\n",
        "      state, r, done = env.step(a)\n",
        "      env.render()\n",
        "      print('Reward: {0}.'.format(r))\n",
        "      if done: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42VoVYBMgub",
        "outputId": "5fe5c961-6c62-4835-b94f-7a9f09664773"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS FROM SARSA\n",
            "Policy:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "State values:\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.27043522 0.        ]\n",
            "------------------------------------\n",
            "RESULTS FROM Q-LEARNING\n",
            "Policy:\n",
            "[3 3 2 1 0 0 2 0 3 3 2 0 0 0 3 0 0]\n",
            "State values:\n",
            "[0.0304413  0.03438092 0.04063816 0.03356059 0.02591354 0.\n",
            " 0.04604886 0.         0.02314196 0.03030605 0.06133057 0.\n",
            " 0.         0.0176214  0.10354972 0.38955867 0.        ]\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER SARSA\n",
            "\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['@' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "\n",
            "================================================\n",
            "\n",
            "AGENT PERFORMANCE AFTER Q-LEARNING\n",
            "\n",
            "[['&' '@' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['&' '.' '@' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '@' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '@' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Reward: 0.\n"
          ]
        }
      ]
    }
  ]
}
