{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2, Main (i.e. running all the functions)"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the context\n",
        "The following are the necessary preparations and imports needed to run and test the main code of this document in the intended context. Mounting directory & setting present working directory..."
      ],
      "metadata": {
        "id": "PG_fIXd_8SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the Google Drive folder (run if necessary):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "# Saving the present working directory's path:\n",
        "# NOTE: Change `pwd` based on your own Google Drive organisation\n",
        "pwd = \"./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke40qWkgzGIO",
        "outputId": "1a126790-b529-4f84-9c25-0e156948414e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install module `import_ipynb` to enable importing Jupyter Notebooks as modules...\n",
        "\n",
        "`!pip install import_ipynb`\n",
        "\n",
        "Importing the code in notebook `Q1_environment.ipynb`...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm_Q5HCM1Rf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "N = import_ipynb.NotebookLoader(path=[pwd])\n",
        "N.load_module(\"Q1_environment\")\n",
        "N.load_module(\"Q2_tabularModelBasedMethods\")\n",
        "N.load_module(\"Q3_tabularModelFreeMethods\")\n",
        "N.load_module(\"Q4_nonTabularModelFreeMethods\")\n",
        "N.load_module(\"Q5_deepReinforcementLearning\")\n",
        "from Q1_environment import *\n",
        "from Q2_tabularModelBasedMethods import *\n",
        "from Q3_tabularModelFreeMethods import *\n",
        "from Q4_nonTabularModelFreeMethods import *\n",
        "from Q5_deepReinforcementLearning import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IgTup74k5S",
        "outputId": "aec4ba4e-9abb-4ada-b206-8c07b3e25b6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q1_environment.ipynb\n",
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q2_tabularModelBasedMethods.ipynb\n",
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q3_tabularModelFreeMethods.ipynb\n",
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q4_nonTabularModelFreeMethods.ipynb\n",
            "importing Jupyter notebook from ./drive/MyDrive/ColabNotebooks/AIG-Labs/AIG-Assignment2/Q5_deepReinforcementLearning.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other necessary imports..."
      ],
      "metadata": {
        "id": "Eqid0ZW_qvkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "xEpt9SkPotQo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function"
      ],
      "metadata": {
        "id": "_kEbFhgjg9-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Preliminary definitions:\n",
        "    seed = 0\n",
        "    lake = LAKE['small']\n",
        "    env = FrozenLake(lake, slip=0.1, max_steps=16, seed=seed)\n",
        "    gamma = 0.9\n",
        "    max_episodes = 4000\n",
        "    H1 = '\\n================================================\\n'\n",
        "    H2 = '------------------------------------\\n'\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    print(f'{H1}Model-based algorithms')\n",
        "    '''\n",
        "    ARGUMENTS:\n",
        "    - Environment\n",
        "    - Discount factor\n",
        "    - Error margin for convergence of value function\n",
        "    - Maximum iterations\n",
        "    '''\n",
        "\n",
        "    print(f'{H2}Policy iteration')\n",
        "    args = [env, gamma, 0.001, 128]\n",
        "\n",
        "    policy, value, i = policy_iteration(*args)\n",
        "    env.render(policy, value)\n",
        "\n",
        "    print(f'{H2}Value iteration')\n",
        "    policy, value, i = value_iteration(*args)\n",
        "    env.render(policy, value)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    print(f'{H1}Tabular model-free algorithms')\n",
        "    '''\n",
        "    ARGUMENTS:\n",
        "    - Environment\n",
        "    - Maximum episodes to iterate over (1 episode ==> play until absorbed)\n",
        "    - Initial learning rate\n",
        "    - Discount factor\n",
        "    - Exploration factor (epsilon)\n",
        "    - Pseudorandom number generator seed\n",
        "    '''\n",
        "    args = [env, max_episodes, 0.5, gamma, 1.0, seed]\n",
        "\n",
        "    print(f'{H2}SARSA')\n",
        "    env.resetRandomState()\n",
        "    policy, value = sarsa(*args)\n",
        "    env.render(policy, value)\n",
        "\n",
        "    print(f'{H2}Q-learning')\n",
        "    env.resetRandomState()\n",
        "    policy, value = q_learning(*args)\n",
        "    env.render(policy, value)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    print(f'{H1}Non-tabular model-free algorithms')\n",
        "    # NOTE: Except for environment, all arguments are the same as before\n",
        "    args[0] = LinearWrapper(env)\n",
        "\n",
        "    print(f'{H2}Linear SARSA')\n",
        "    env.resetRandomState()\n",
        "    parameters = linear_sarsa(*args)\n",
        "    policy, value = linear_env.decode_policy(parameters)\n",
        "    args[0].render(policy, value)\n",
        "\n",
        "    print(f'{H2}Linear Q-learning')\n",
        "    env.resetRandomState()\n",
        "    params = linear_q_learning(*args)\n",
        "    policy, value = linear_env.decode_policy(parameters)\n",
        "    args[0].render(policy, value)\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    print(f'{H1}Deep Q-network learning')\n",
        "    # ARGUMENTS:\n",
        "    args = [FrozenLakeImageWrapper(env), # Wrapped environment\n",
        "            max_episodes,                # Maximum episodes\n",
        "            0.001,                       # Learning rate\n",
        "            gamma,                       # Discount factor\n",
        "            0.2,                         # Exploration factor (epsilon)\n",
        "            32,                          # Batch size (random sample size)\n",
        "            4,                           # Target update frequency\n",
        "            256,                         # Replay buffer size\n",
        "            3,                           # Kernel size\n",
        "            4,                           # Convolution layer output channels\n",
        "            8,                           # Fully-connected layer output features\n",
        "            4]                           # Pseudorandom number generator seed\n",
        "\n",
        "    env.resetRandomState()\n",
        "    dqn = deep_q_network_learning(*args)\n",
        "    policy, value = args[0].decode_policy(dqn)\n",
        "    args[0].render(policy, value)\n",
        "\n",
        "# Run function of the current file is the file being executed:\n",
        "if __name__ == '__main__': main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2CBAT2Sg8un",
        "outputId": "611a5949-d1b1-4f8f-9309-5b2a67b55ce1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================\n",
            "Model-based algorithms\n",
            "------------------------------------\n",
            "Policy iteration\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.403 0.469 0.552 0.480]\n",
            " [0.472 0.000 0.637 0.000]\n",
            " [0.555 0.654 0.751 0.000]\n",
            " [0.000 0.737 0.867 1.000]]\n",
            "------------------------------------\n",
            "Value iteration\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.455 0.504 0.579 0.505]\n",
            " [0.508 0.000 0.653 0.000]\n",
            " [0.584 0.672 0.768 0.000]\n",
            " [0.000 0.771 0.887 1.000]]\n",
            "\n",
            "================================================\n",
            "Tabular model-free algorithms\n",
            "------------------------------------\n",
            "SARSA\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.411 0.361 0.451 0.306]\n",
            " [0.463 0.000 0.559 0.000]\n",
            " [0.558 0.664 0.754 0.000]\n",
            " [0.000 0.768 0.892 1.000]]\n",
            "------------------------------------\n",
            "Q-learning\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '_' '_' '^']\n",
            " ['^' '>' '>' '>']]\n",
            "Value:\n",
            "[[0.453 0.473 0.564 0.524]\n",
            " [0.509 0.000 0.616 0.000]\n",
            " [0.578 0.681 0.726 0.000]\n",
            " [0.000 0.768 0.873 1.000]]\n",
            "\n",
            "================================================\n",
            "Non-tabular model-free algorithms\n",
            "------------------------------------\n",
            "Linear SARSA\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.399 0.345 0.439 0.311]\n",
            " [0.452 0.000 0.618 0.000]\n",
            " [0.550 0.648 0.744 0.000]\n",
            " [0.000 0.773 0.889 1.000]]\n",
            "------------------------------------\n",
            "Linear Q-learning\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '>' '_' '<']\n",
            " ['_' '^' '_' '^']\n",
            " ['>' '>' '_' '^']\n",
            " ['^' '>' '>' '^']]\n",
            "Value:\n",
            "[[0.399 0.345 0.439 0.311]\n",
            " [0.452 0.000 0.618 0.000]\n",
            " [0.550 0.648 0.744 0.000]\n",
            " [0.000 0.773 0.889 1.000]]\n",
            "\n",
            "================================================\n",
            "Deep Q-network learning\n",
            "Lake:\n",
            "[['&' '.' '.' '.']\n",
            " ['.' '#' '.' '#']\n",
            " ['.' '.' '.' '#']\n",
            " ['#' '.' '.' '$']]\n",
            "Policy:\n",
            "[['_' '<' '_' '<']\n",
            " ['_' '<' '_' '<']\n",
            " ['>' '>' '_' '<']\n",
            " ['<' '>' '>' '<']]\n",
            "Value:\n",
            "[[ 0.471  0.418  0.421  0.410]\n",
            " [ 0.497  0.011  0.608 -0.006]\n",
            " [ 0.607  0.710  0.789 -0.010]\n",
            " [ 0.004  0.807  0.908  1.009]]\n"
          ]
        }
      ]
    }
  ]
}