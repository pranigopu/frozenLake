{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI in Games, _Reinforcement Learning_<br>Assignment 2"
      ],
      "metadata": {
        "id": "Ktw7o35u_sHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import contextlib\n",
        "\n",
        "# Configures numpy print options\n",
        "@contextlib.contextmanager\n",
        "def _printoptions(*args, **kwargs):\n",
        "    original = np.get_printoptions()\n",
        "    np.set_printoptions(*args, **kwargs)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        np.set_printoptions(**original)"
      ],
      "metadata": {
        "id": "VrK-GtTF_5m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "9KI47vCojmJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy evaluation"
      ],
      "metadata": {
        "id": "HkcS9Sled6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma, theta, max_iterations):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    env: object of the chosen environment class (ex. FrozenLake)\n",
        "    policy: array giving the probability of taking an action from a state\n",
        "    gamma: discount factor\n",
        "    theta: error tolerance level\n",
        "\n",
        "    FURTHER NOTE ON `policy`:\n",
        "    We consider the policy to be deterministic, meaning that it maps each state\n",
        "    to a certain action, rather than each state-action pair to a probability.\n",
        "    Hence, `policy` is a 1D array with each index corresponding to a state and\n",
        "    the value at a given index i corresponding to the action to be taken from\n",
        "    state i. This is equivalent to the policy wherein each state-action pair is\n",
        "    related to either 0 or 1 so that each state is mapped to 1 only when paired\n",
        "    with a particular action.\n",
        "    '''\n",
        "    # Initialising table of values per state:\n",
        "    value = np.zeros(env.n_states, dtype=np.float)\n",
        "    # Flag for indicating convergence of value evaluation:\n",
        "    flag = 0\n",
        "    # Policy evaluation loop:\n",
        "    for i in range(max_iterations):\n",
        "        for s in range(env.n_states):\n",
        "            # NOTE: s ==> state\n",
        "            # Storing previous value of value function:\n",
        "            v = value[s]\n",
        "            #------------------------------------\n",
        "            # Obtaining current value of value function:\n",
        "            # NOTE: We iterate through every possible action from state `s`\n",
        "            value[s] = 0\n",
        "            for a in range(env.n_actions):\n",
        "                # NOTE: a ==> action\n",
        "                # If policy does not map s to a, move to next action:\n",
        "                if policy[s] != a: continue\n",
        "                # If policy does map s to a, update state value:\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    value[s] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    env.p: Probability of moving from s to _s given action a\n",
        "                    env.r: Reward of moving from s to _s given action a\n",
        "                    '''\n",
        "            #------------------------------------\n",
        "            # Obtaining the difference in state value:\n",
        "            # NOTE: This is why we stored the previous value before in `v`\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "    return value"
      ],
      "metadata": {
        "id": "UedO36FhIPP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy improvement"
      ],
      "metadata": {
        "id": "roJtJVb5d87b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(env, value, gamma):\n",
        "    '''\n",
        "    NOTE ON THE ARGUMENTS:\n",
        "    env: object of the chosen environment class (ex. FrozenLake)\n",
        "    value: array containing values of each state with respect to some policy\n",
        "    gamma: discount factor\n",
        "\n",
        "    NOTE ON POLICY IMPROVEMENT:\n",
        "    The goal of policy improvement is to improve on the previously used policy\n",
        "    (which is implicit in the array of state values, which is evaluated with\n",
        "    respect to some policy). We do this by choosing for each state s the action\n",
        "    a such that we maximise the reward of taking a from s (irrespective of\n",
        "    policy) then following the previous policy (which is implicit in the array\n",
        "    of state values).\n",
        "    '''\n",
        "    policy = np.zeros(env.n_states, dtype=int)\n",
        "    for s in range(env.n_states):\n",
        "        q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "        for a in range(env.n_actions):\n",
        "            for _s in range(env.n_states):\n",
        "                # NOTE: _s ==> next state\n",
        "                # Total reward of taking a from s then following last policy:\n",
        "                '''\n",
        "                NOTE ON LAST POLICY:\n",
        "                The previous policy based on which we are making the current\n",
        "                improvement is implicit in the array of state values `value`,\n",
        "                since this array was obtained with respect to some policy.\n",
        "                '''\n",
        "                q[a] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                '''\n",
        "                NOTE ON ABOVE USED FUNCTIONS:\n",
        "                env.p: Probability of moving from s to _s given action a\n",
        "                env.r: Reward of moving from s to _s given action a\n",
        "                '''\n",
        "        # Update policy to maximise the one-step dynamics from s:\n",
        "        policy[s] = np.argmax(q)\n",
        "    return policy"
      ],
      "metadata": {
        "id": "A4D_aCb4dvhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy iteration..."
      ],
      "metadata": {
        "id": "gO_Ul57neABk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma, theta, max_iterations, policy=None):\n",
        "    if policy is None: policy = np.zeros(env.n_states, dtype=int)\n",
        "    else: policy = np.array(policy, dtype=int)\n",
        "    # Initialising state values with respect to existing policy:\n",
        "    value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "    # Policy iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        policy = policy_improvement(env, value, gamma)\n",
        "        new_value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "        # If all value evaluations change less than theta, break:\n",
        "        if all(abs(new_value-value) < theta): break\n",
        "        # Else, continue improving with the newly evaluated state values:\n",
        "        value = new_value\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "kWqlg13zWAxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value iteration"
      ],
      "metadata": {
        "id": "w4oRY11heB7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma, theta, max_iterations, value=None):\n",
        "    if value is None: value = np.zeros(env.n_states)\n",
        "    else: value = np.array(value, dtype=np.float)\n",
        "    # Flag for indicating convergence of value evaluation:\n",
        "    flag = 0\n",
        "    # Value iteration loop:\n",
        "    for i in range(max_iterations):\n",
        "        for s in range(env.n_states):\n",
        "            q = np.zeros(env.n_actions, dtype=np.float32)\n",
        "            for a in range(env.n_actions):\n",
        "                for _s in range(env.n_states):\n",
        "                    # NOTE: _s ==> next state\n",
        "                    # Total reward of taking a from s for previous state value:\n",
        "                    q[a] += env.p(_s,s,a)*(env.r(_s,s,a) + gamma*value[_s])\n",
        "                    '''\n",
        "                    NOTE ON ABOVE USED FUNCTIONS:\n",
        "                    env.p: Probability of moving from s to _s given action a\n",
        "                    env.r: Reward of moving from s to _s given action a\n",
        "                    '''\n",
        "            # Update policy to maximise the one-step dynamics from s:\n",
        "            v = value[s]\n",
        "            value[s] = np.max(q)\n",
        "            if abs(value[s]-v) < theta: flag += 1\n",
        "\n",
        "        # If difference in state value < theta for all states, stop:\n",
        "        if flag == env.n_states: break\n",
        "\n",
        "    #================================================\n",
        "\n",
        "    # Obtaining the (maybe estimated) optimal policy:\n",
        "    # NOTE: The logic for this is identical to policy improvement\n",
        "    policy = policy_improvement(env, value, gamma)\n",
        "    return policy, value"
      ],
      "metadata": {
        "id": "zOCfwm_0d166"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
